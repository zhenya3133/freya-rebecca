From 6796af8663907653cfd7133874d2d3d04e9dbad8 Mon Sep 17 00:00:00 2001
From: zhenya3133 <verbenkoe@gmail.com>
Date: Tue, 23 Sep 2025 19:40:55 +0300
Subject: [PATCH 04/14] chore(restore): restore CI, scripts, docs; mark scripts
 executable

---
 .github/PULL_REQUEST_TEMPLATE.md              |  12 ++
 .github/workflows/ci.yml                      |  79 ++++++++++
 .gitignore                                    |  16 ++
 Makefile                                      | 126 +++++++++++++++
 apps/web/docs/CHANGELOG.md                    |   0
 apps/web/docs/evals/latest.md                 |  13 ++
 apps/web/docs/retrieve-api.md                 |  15 ++
 apps/web/scripts/audit-state.mjs              |   0
 apps/web/scripts/contract/test_retrieve.sh    |  22 +++
 apps/web/scripts/db/maintenance.sh            |   4 +
 apps/web/scripts/db/maintenance.sql           |   5 +
 apps/web/scripts/diag/count_chunks.sql        |   6 +
 apps/web/scripts/diag/count_chunks_ns.ts      |  26 ++++
 apps/web/scripts/diag/count_memories.sql      |   9 ++
 apps/web/scripts/diag/count_memories_ns.ts    |  26 ++++
 apps/web/scripts/diag/count_ns.sql            |  34 +++++
 apps/web/scripts/diag/count_ns.ts             |  31 ++++
 apps/web/scripts/diag/list_columns.sh         |  19 +++
 apps/web/scripts/diag/list_columns.sql        |  15 ++
 apps/web/scripts/diag/list_columns.ts         |  27 ++++
 apps/web/scripts/diag/list_tables.sh          |  13 ++
 apps/web/scripts/diag/list_tables.sql         |   8 +
 apps/web/scripts/diag/slots_by_ns.ts          |  24 +++
 apps/web/scripts/diag/top_ns.ts               |  31 ++++
 apps/web/scripts/e2e/bootstrap_demo.sh        |  81 ++++++++++
 .../evals/golden/rebecca_army_refs.jsonl      |   8 +
 apps/web/scripts/evals/run_eval.mjs           | 133 ++++++++++++++++
 apps/web/scripts/evals/run_eval.ts            | 144 ++++++++++++++++++
 apps/web/scripts/evals/sample_cases.jsonl     |   5 +
 apps/web/scripts/ingest_github_paged.sh       | 130 ++++++++++++++++
 apps/web/scripts/ingest_readme_pdfs.sh        |  59 +++++++
 apps/web/scripts/migrate-g0.js                |   0
 apps/web/scripts/migrate.sh                   |  37 +++++
 apps/web/scripts/migrations/G0_init.sql       |  24 +++
 .../scripts/migrations/G5_indexes_upserts.sql |  17 +++
 .../g5_backfill_chunks_from_memories.sql      |  19 +++
 36 files changed, 1218 insertions(+)
 create mode 100644 .github/PULL_REQUEST_TEMPLATE.md
 create mode 100644 .github/workflows/ci.yml
 create mode 100644 Makefile
 mode change 100644 => 100755 apps/web/docs/CHANGELOG.md
 create mode 100644 apps/web/docs/evals/latest.md
 create mode 100644 apps/web/docs/retrieve-api.md
 mode change 100644 => 100755 apps/web/scripts/audit-state.mjs
 create mode 100644 apps/web/scripts/contract/test_retrieve.sh
 create mode 100755 apps/web/scripts/db/maintenance.sh
 create mode 100644 apps/web/scripts/db/maintenance.sql
 create mode 100644 apps/web/scripts/diag/count_chunks.sql
 create mode 100644 apps/web/scripts/diag/count_chunks_ns.ts
 create mode 100644 apps/web/scripts/diag/count_memories.sql
 create mode 100644 apps/web/scripts/diag/count_memories_ns.ts
 create mode 100644 apps/web/scripts/diag/count_ns.sql
 create mode 100644 apps/web/scripts/diag/count_ns.ts
 create mode 100755 apps/web/scripts/diag/list_columns.sh
 create mode 100644 apps/web/scripts/diag/list_columns.sql
 create mode 100644 apps/web/scripts/diag/list_columns.ts
 create mode 100755 apps/web/scripts/diag/list_tables.sh
 create mode 100644 apps/web/scripts/diag/list_tables.sql
 create mode 100644 apps/web/scripts/diag/slots_by_ns.ts
 create mode 100644 apps/web/scripts/diag/top_ns.ts
 create mode 100755 apps/web/scripts/e2e/bootstrap_demo.sh
 create mode 100644 apps/web/scripts/evals/golden/rebecca_army_refs.jsonl
 create mode 100644 apps/web/scripts/evals/run_eval.mjs
 create mode 100644 apps/web/scripts/evals/run_eval.ts
 create mode 100644 apps/web/scripts/evals/sample_cases.jsonl
 create mode 100755 apps/web/scripts/ingest_github_paged.sh
 create mode 100755 apps/web/scripts/ingest_readme_pdfs.sh
 mode change 100644 => 100755 apps/web/scripts/migrate-g0.js
 create mode 100755 apps/web/scripts/migrate.sh
 create mode 100644 apps/web/scripts/migrations/G0_init.sql
 create mode 100644 apps/web/scripts/migrations/G5_indexes_upserts.sql
 create mode 100644 apps/web/scripts/migrations/g5_backfill_chunks_from_memories.sql

diff --git a/.github/PULL_REQUEST_TEMPLATE.md b/.github/PULL_REQUEST_TEMPLATE.md
new file mode 100644
index 0000000..6fc3fe0
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE.md
@@ -0,0 +1,12 @@
+## Checklist
+
+- [ ] Contract retrieval RC v1 не менялся
+  - [ ] Если менялся: обновлены docs/contract, миграции, примеры
+- [ ] Ingest upsert (chunks) зелёный, идемпотентность сохранена
+- [ ] Eval прошёл локально: `npx tsx apps/web/scripts/evals/run_eval.ts` → hit@5 > 0
+- [ ] Диаг-скрипты выполняются: list_tables/list_columns/count_ns/count_chunks
+- [ ] CI зелёный: typecheck, eslint, bash -n, (и integration если DATABASE_URL задан)
+- [ ] .env.local не закоммичен, env.example актуален
+
+## Notes
+Коротко, что поменяли и почему.
diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
new file mode 100644
index 0000000..5843830
--- /dev/null
+++ b/.github/workflows/ci.yml
@@ -0,0 +1,79 @@
+name: CI
+
+on:
+  pull_request:
+  push:
+    branches: [ main ]
+
+jobs:
+  ci:
+    runs-on: ubuntu-latest
+
+    env:
+      NODE_ENV: production
+      # опц: если задашь в репозитории -> пробежит интеграционный блок
+      DATABASE_URL: ${{ secrets.DATABASE_URL }}
+      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+      X_ADMIN_KEY: test_admin_key_123
+      RETRIEVE_ALPHA: "0.85"
+      RETRIEVE_BETA: "0.15"
+      RETRIEVE_T_HALF: "180"
+      RETRIEVE_PROBES: "10"
+      BASE: http://localhost:3000
+      NS: rebecca/army/refs
+      SLOT: staging
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Use Node 20
+        uses: actions/setup-node@v4
+        with:
+          node-version: 20
+
+      - name: Set up pnpm
+        uses: pnpm/action-setup@v4
+        with:
+          version: 9
+
+      - name: Install deps (root & apps/web)
+        run: |
+          pnpm install --frozen-lockfile || pnpm install
+
+      # 1) типы + линт
+      - name: Typecheck
+        run: pnpm -w --filter ./apps/web tsc --noEmit
+
+      - name: ESLint
+        run: pnpm -w --filter ./apps/web eslint --max-warnings=0 .
+
+      # 2) shell scripts syntax
+      - name: Bash syntax
+        run: |
+          find apps/web/scripts -type f -name "*.sh" -print0 | xargs -0 -I{} bash -n "{}"
+
+      # 3) интеграционные: только если есть DATABASE_URL
+      - name: Build Next app
+        if: env.DATABASE_URL != ''
+        working-directory: apps/web
+        run: pnpm build
+
+      - name: Start Next app
+        if: env.DATABASE_URL != ''
+        working-directory: apps/web
+        run: |
+          pnpm start &>/tmp/next.log &
+          npx wait-on http://localhost:3000
+
+      - name: Contract test /api/retrieve
+        if: env.DATABASE_URL != ''
+        run: |
+          chmod +x apps/web/scripts/contract/test_retrieve.sh
+          apps/web/scripts/contract/test_retrieve.sh
+
+      # 4) отчёт логов в случае падения
+      - name: Print Next logs on failure
+        if: failure()
+        run: |
+          echo "==== NEXT LOG ===="
+          tail -n +1 /tmp/next.log || true
diff --git a/.gitignore b/.gitignore
index 0470db0..1599490 100644
--- a/.gitignore
+++ b/.gitignore
@@ -16,3 +16,19 @@ Thumbs.db
 # runtime payload
 apps/web/answer.json
  .env.local 
+
+# local/env
+apps/web/.env.local
+
+# bundles & tmp
+apps/web/_sanity_bundle/
+apps/web/sanity-bundle-*.tar.gz
+apps/web/eval_reports/
+*.log
+.DS_Store
+apps/web/.env.local
+apps/web/_sanity_bundle/
+apps/web/sanity-bundle-*.tar.gz
+apps/web/eval_reports/
+*.log
+.DS_Store
diff --git a/Makefile b/Makefile
new file mode 100644
index 0000000..2b05d55
--- /dev/null
+++ b/Makefile
@@ -0,0 +1,126 @@
+# ---------- Makefile (root) ----------
+SHELL := /bin/bash
+.ONESHELL:
+.DEFAULT_GOAL := help
+
+# Параметры по умолчанию (можно переопределять: make ingest:demo NS=foo SLOT=prod)
+WEB_DIR   ?= apps/web
+ENV_FILE  ?= $(WEB_DIR)/.env.local
+BASE      ?= http://localhost:3000
+NS        ?= rebecca/army/refs
+SLOT      ?= staging
+LOCAL_PDF ?= file:///mnt/c/Users/User/Desktop/Mastering\ AI\ Agents-compressed.pdf
+
+# Шорткат: подхватить переменные из .env.local для каждого шага
+define WITH_ENV
+set -euo pipefail; \
+if [[ -f "$(ENV_FILE)" ]]; then set -a; source "$(ENV_FILE)"; set +a; fi; \
+$$@
+endef
+
+.PHONY: help
+help: ## Показать список целей
+	@awk 'BEGIN {FS = ":.*##"; printf "\nTargets:\n"} /^[a-zA-Z0-9_:%-]+:.*##/ {printf "  \033[36m%-22s\033[0m %s\n", $$1, $$2}' $(MAKEFILE_LIST)
+
+# ------------------------ Миграции ------------------------
+
+.PHONY: migrate
+migrate: ## Прогнать миграции (G0 + G5)
+	@$(WITH_ENV) $(WEB_DIR)/scripts/migrate.sh
+
+# ------------------------ Диагностика ---------------------
+
+.PHONY: diag:tables
+diag:tables: ## Показать таблицы
+	@$(WITH_ENV) $(WEB_DIR)/scripts/diag/list_tables.sh
+
+.PHONY: diag:columns
+diag:columns: ## Показать колонки таблицы: make diag:columns T=chunks
+	@if [[ -z "$$T" ]]; then echo "Usage: make diag:columns T=<table>"; exit 1; fi
+	@$(WITH_ENV) $(WEB_DIR)/scripts/diag/list_columns.sh "$$T"
+
+.PHONY: diag:ns
+diag:ns: ## Сводка по ns (по таблице chunks). Опц.: make diag:ns NS=...
+	@$(WITH_ENV) psql "$$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off \
+	  -v ns='$(NS)' -f $(WEB_DIR)/scripts/diag/count_ns.sql
+
+.PHONY: diag:chunks
+diag:chunks: ## Подсчет по chunks
+	@$(WITH_ENV) psql "$$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off \
+	  -f $(WEB_DIR)/scripts/diag/count_chunks.sql
+
+.PHONY: diag:memories
+diag:memories: ## Подсчет по memories (временный до удаления)
+	@$(WITH_ENV) psql "$$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off \
+	  -f $(WEB_DIR)/scripts/diag/count_memories.sql
+
+# ------------------------ БД утилиты ----------------------
+
+.PHONY: db:init
+db:init: migrate ## Инициализация базы (миграции)
+
+.PHONY: db:reset
+db:reset: ## TRUNCATE chunks для заданных NS/SLOT + ANALYZE
+	@$(WITH_ENV) psql "$$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off -c \
+	  "DELETE FROM chunks WHERE ns='$(NS)' AND slot='$(SLOT)';"
+	@$(WITH_ENV) psql "$$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off -c \
+	  "ANALYZE chunks;"
+
+# ------------------------ Ingest / E2E --------------------
+
+.PHONY: ingest:demo
+ingest:demo: ## Полная загрузка демо-корпуса (URL+PDF+GitHub) и ANALYZE
+	@$(WITH_ENV) LOCAL_PDF="$(LOCAL_PDF)" $(WEB_DIR)/scripts/e2e/bootstrap_demo.sh
+
+.PHONY: ingest:url
+ingest:url: ## Пример: make ingest:url Q="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop"
+	@if [[ -z "$$Q" ]]; then echo "Usage: make ingest:url Q=<url>"; exit 1; fi
+	@$(WITH_ENV) curl -sS -X POST "$(BASE)/api/ingest/url" \
+	  -H 'content-type: application/json' -H "x-admin-key: $$X_ADMIN_KEY" \
+	  -d "{\"ns\":\"$(NS)\",\"slot\":\"$(SLOT)\",\"urls\":[\"$$Q\"]}" | jq
+
+.PHONY: ingest:pdf
+ingest:pdf: ## Пример: make ingest:pdf Q="file:///abs/path/file.pdf"
+	@if [[ -z "$$Q" ]]; then echo "Usage: make ingest:pdf Q=<url|file://...>"; exit 1; fi
+	@$(WITH_ENV) curl -sS -X POST "$(BASE)/api/ingest/pdf" \
+	  -H 'content-type: application/json' -H "x-admin-key: $$X_ADMIN_KEY" \
+	  -d "{\"ns\":\"$(NS)\",\"slot\":\"$(SLOT)\",\"url\":\"$$Q\"}" | jq
+
+.PHONY: ingest:gh
+ingest:gh: ## Пример: make ingest:gh OWNER=openai REPO=openai-cookbook LIMIT=10
+	@if [[ -z "$$OWNER" || -z "$$REPO" ]]; then echo "Usage: make ingest:gh OWNER=<o> REPO=<r> [LIMIT=10]"; exit 1; fi
+	@$(WITH_ENV) curl -sS -X POST "$(BASE)/api/ingest/github" \
+	  -H 'content-type: application/json' -H "x-admin-key: $$X_ADMIN_KEY" \
+	  -d "{\"ns\":\"$(NS)\",\"slot\":\"$(SLOT)\",\"owner\":\"$$OWNER\",\"repo\":\"$$REPO\",\"includeExt\":[\".md\"],\"limit\":$${LIMIT:-10}}" | jq
+
+# ------------------------ Retrieve / Eval -----------------
+
+.PHONY: retrieve:test
+retrieve:test: ## Два sanity-запроса к /api/retrieve (basic + allow-domain)
+	@$(WITH_ENV) BASE="$(BASE)" NS="$(NS)" SLOT="$(SLOT)" \
+	  $(WEB_DIR)/scripts/examples/retrieve.sh
+
+.PHONY: eval
+eval: ## Запустить минимальный Golden eval, отчёт -> apps/web/docs/evals/latest.md
+	@$(WITH_ENV) BASE="$(BASE)" NS="$(NS)" SLOT="$(SLOT)" \
+	  npx -y tsx $(WEB_DIR)/scripts/evals/run_eval.ts | jq
+
+# ------------------------ Локальный CI --------------------
+
+.PHONY: ci
+ci: ## Локальный CI: tsc, eslint, bash -n, контрактный тест /api/retrieve
+	@echo ">> tsc --noEmit"
+	@npx -y tsc --noEmit
+	@echo ">> eslint"
+	@npx -y eslint . --max-warnings=0
+	@echo ">> bash -n"
+	@bash -n $(WEB_DIR)/scripts/e2e/bootstrap_demo.sh
+	@bash -n $(WEB_DIR)/scripts/diag/list_tables.sh
+	@bash -n $(WEB_DIR)/scripts/diag/list_columns.sh
+	@bash -n $(WEB_DIR)/scripts/migrate.sh
+	@echo ">> contract smoke: /api/retrieve"
+	@$(WITH_ENV) curl -sS -X POST "$(BASE)/api/retrieve" -H 'content-type: application/json' \
+	  -d "{\"q\":\"ping\",\"ns\":\"$(NS)\",\"slot\":\"$(SLOT)\",\"topK\":1,\"candidateK\":1,\"minSimilarity\":0,\"nsMode\":\"strict\"}" \
+	  | jq '.items!=null' | grep true >/dev/null
+
+# ----------------------------------------------------------
diff --git a/apps/web/docs/CHANGELOG.md b/apps/web/docs/CHANGELOG.md
old mode 100644
new mode 100755
diff --git a/apps/web/docs/evals/latest.md b/apps/web/docs/evals/latest.md
new file mode 100644
index 0000000..c204411
--- /dev/null
+++ b/apps/web/docs/evals/latest.md
@@ -0,0 +1,13 @@
+# Eval report
+Date: 2025-09-23T14:13:50.292Z
+NS: `rebecca/army/refs`, slot: `staging`, base: `http://localhost:3000`
+
+**Summary:** n=5, hit@k=1.000, MRR=1.000
+
+| # | query | rank | top URLs |
+|--:|-------|-----:|----------|
+| 1 | event loop microtask | 1 | developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop<br>developer.mozilla.org/en-US/docs/Web/API/console/time<br>developer.mozilla.org/en-US/docs/Web/API/Console/table |
+| 2 | console.time usage | 1 | developer.mozilla.org/en-US/docs/Web/API/console/time<br>developer.mozilla.org/en-US/docs/Web/API/Console/table<br>developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop |
+| 3 | console.table example | 1 | developer.mozilla.org/en-US/docs/Web/API/Console/table<br>developer.mozilla.org/en-US/docs/Web/API/console/time<br>developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop |
+| 4 | openai cookbook examples | 1 | github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/verifying-implementations.md<br>github.com/openai/openai-cookbook/blob/main/.github/pull_request_template.md<br>github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-locally-lmstudio.md |
+| 5 | Mastering AI Agents | 1 | file:///mnt/c/Users/User/Desktop/Mastering AI Agents-compressed.pdf<br>github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-vllm.md<br>github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/run-locally-ollama.md |
diff --git a/apps/web/docs/retrieve-api.md b/apps/web/docs/retrieve-api.md
new file mode 100644
index 0000000..9374e7e
--- /dev/null
+++ b/apps/web/docs/retrieve-api.md
@@ -0,0 +1,15 @@
+# /api/retrieve — мини-README
+
+**Метод:** `POST`
+
+## Тело запроса
+```json
+{
+  "q": "строка запроса",
+  "ns": "rebecca/army/refs",
+  "topK": 10,
+  "domainFilter": { "allow": ["developer.mozilla.org"], "deny": ["wikipedia.org"] },
+  "debug": true
+}
+
+
diff --git a/apps/web/scripts/audit-state.mjs b/apps/web/scripts/audit-state.mjs
old mode 100644
new mode 100755
diff --git a/apps/web/scripts/contract/test_retrieve.sh b/apps/web/scripts/contract/test_retrieve.sh
new file mode 100644
index 0000000..c48f51d
--- /dev/null
+++ b/apps/web/scripts/contract/test_retrieve.sh
@@ -0,0 +1,22 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+BASE="${BASE:-http://localhost:3000}"
+
+pass() { echo "OK  - $1"; }
+fail() { echo "FAIL- $1"; exit 1; }
+
+# 1) valid body
+valid='{"q":"ping","ns":"rebecca/army/refs","slot":"staging","topK":1,"candidateK":1,"minSimilarity":0,"nsMode":"strict"}'
+resp_valid="$(curl -sS -X POST "$BASE/api/retrieve" -H 'content-type: application/json' -d "$valid")" || fail "valid POST failed (curl)"
+echo "$resp_valid" | jq -e '.items != null' >/dev/null 2>&1 && pass "valid JSON accepted" || fail "valid JSON not accepted"
+
+# 2) invalid body (нет q)
+invalid='{"ns":"rebecca/army/refs","slot":"staging","topK":1}'
+resp_invalid="$(curl -sS -X POST "$BASE/api/retrieve" -H 'content-type: application/json' -d "$invalid" || true)"
+echo "$resp_invalid" | jq -e 'has("error")' >/dev/null 2>&1 && pass "invalid JSON rejected with error" || fail "invalid JSON passed unexpectedly"
+
+# 3) optional domain allow
+with_domain='{"q":"event loop","ns":"rebecca/army/refs","slot":"staging","topK":3,"candidateK":100,"minSimilarity":0,"nsMode":"prefix","domainFilter":{"allow":["developer.mozilla.org"]}}'
+resp_domain="$(curl -sS -X POST "$BASE/api/retrieve" -H 'content-type: application/json' -d "$with_domain")" || fail "domain POST failed"
+echo "$resp_domain" | jq -e '.items != null' >/dev/null 2>&1 && pass "domain allow accepted" || fail "domain allow not accepted"
diff --git a/apps/web/scripts/db/maintenance.sh b/apps/web/scripts/db/maintenance.sh
new file mode 100755
index 0000000..8da9fa7
--- /dev/null
+++ b/apps/web/scripts/db/maintenance.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+set -euo pipefail
+: "${DATABASE_URL:?DATABASE_URL is required}"
+psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off -f "$(dirname "$0")/maintenance.sql"
diff --git a/apps/web/scripts/db/maintenance.sql b/apps/web/scripts/db/maintenance.sql
new file mode 100644
index 0000000..8b32323
--- /dev/null
+++ b/apps/web/scripts/db/maintenance.sql
@@ -0,0 +1,5 @@
+-- ANALYZE target
+ANALYZE public.chunks;
+
+-- опционально: вакуум
+-- VACUUM (ANALYZE) public.chunks;
diff --git a/apps/web/scripts/diag/count_chunks.sql b/apps/web/scripts/diag/count_chunks.sql
new file mode 100644
index 0000000..f419bd2
--- /dev/null
+++ b/apps/web/scripts/diag/count_chunks.sql
@@ -0,0 +1,6 @@
+\pset pager off
+SELECT ns, slot, COUNT(*) AS cnt
+FROM chunks
+GROUP BY ns, slot
+ORDER BY cnt DESC, ns, slot
+LIMIT 200;
diff --git a/apps/web/scripts/diag/count_chunks_ns.ts b/apps/web/scripts/diag/count_chunks_ns.ts
new file mode 100644
index 0000000..bc34766
--- /dev/null
+++ b/apps/web/scripts/diag/count_chunks_ns.ts
@@ -0,0 +1,26 @@
+// scripts/diag/count_chunks_ns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+
+const pool = new Pool({ connectionString: url });
+
+async function main() {
+  const ns = process.env.NS || "rebecca/army/refs";
+  const like = ns.endsWith("%") ? ns : `${ns}%`;
+  const sql = `
+    select ns, count(*)::int as cnt
+    from chunks
+    where ns like $1
+    group by 1
+    order by cnt desc
+    limit 50
+  `;
+  const r = await pool.query(sql, [like]);
+  console.table(r.rows);
+  await pool.end();
+}
+main().catch(e => { console.error("DB error:", e); process.exit(1); });
diff --git a/apps/web/scripts/diag/count_memories.sql b/apps/web/scripts/diag/count_memories.sql
new file mode 100644
index 0000000..d91f223
--- /dev/null
+++ b/apps/web/scripts/diag/count_memories.sql
@@ -0,0 +1,9 @@
+\pset pager off
+SELECT
+  ns,
+  COALESCE(slot,'staging') AS slot,
+  COUNT(*) AS cnt
+FROM memories
+GROUP BY ns, COALESCE(slot,'staging')
+ORDER BY cnt DESC, ns, slot
+LIMIT 200;
diff --git a/apps/web/scripts/diag/count_memories_ns.ts b/apps/web/scripts/diag/count_memories_ns.ts
new file mode 100644
index 0000000..0122686
--- /dev/null
+++ b/apps/web/scripts/diag/count_memories_ns.ts
@@ -0,0 +1,26 @@
+// scripts/diag/count_memories_ns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+
+const pool = new Pool({ connectionString: url });
+
+async function main() {
+  const ns = process.env.NS || "rebecca/army/refs";
+  const like = ns.endsWith("%") ? ns : `${ns}%`;
+  const sql = `
+    select ns, count(*)::int as cnt
+    from memories
+    where ns like $1
+    group by 1
+    order by cnt desc
+    limit 50
+  `;
+  const r = await pool.query(sql, [like]);
+  console.table(r.rows);
+  await pool.end();
+}
+main().catch(e => { console.error("DB error:", e); process.exit(1); });
diff --git a/apps/web/scripts/diag/count_ns.sql b/apps/web/scripts/diag/count_ns.sql
new file mode 100644
index 0000000..1ae36cc
--- /dev/null
+++ b/apps/web/scripts/diag/count_ns.sql
@@ -0,0 +1,34 @@
+-- apps/web/scripts/diag/count_ns.sql
+\pset pager off
+
+-- if :ns не передан, используем пустую строку (значит "все ns")
+\if :{?ns}
+\else
+\set ns ''
+\endif
+
+-- добавим % в конец для префикс-поиска
+\set ns_pat :ns '%'
+
+-- 1) разрез по ns
+SELECT
+  ns,
+  COUNT(*)                                                     AS total,
+  COUNT(*) FILTER (WHERE slot='staging')                      AS staging,
+  COUNT(*) FILTER (WHERE slot='prod')                         AS prod,
+  COUNT(*) FILTER (WHERE url ILIKE '%developer.mozilla.org%') AS mdn,
+  COUNT(*) FILTER (WHERE url ILIKE '%arxiv.org%')             AS arxiv
+FROM chunks
+WHERE (:'ns' = '' OR ns LIKE :'ns_pat')
+GROUP BY ns
+ORDER BY ns;
+
+-- 2) общий итог
+SELECT
+  COUNT(*)                                                     AS total,
+  COUNT(*) FILTER (WHERE slot='staging')                      AS staging,
+  COUNT(*) FILTER (WHERE slot='prod')                         AS prod,
+  COUNT(*) FILTER (WHERE url ILIKE '%developer.mozilla.org%') AS mdn,
+  COUNT(*) FILTER (WHERE url ILIKE '%arxiv.org%')             AS arxiv
+FROM chunks
+WHERE (:'ns' = '' OR ns LIKE :'ns_pat');
diff --git a/apps/web/scripts/diag/count_ns.ts b/apps/web/scripts/diag/count_ns.ts
new file mode 100644
index 0000000..413d6e8
--- /dev/null
+++ b/apps/web/scripts/diag/count_ns.ts
@@ -0,0 +1,31 @@
+// scripts/diag/count_ns.ts
+// Запуск: node --experimental-strip-types scripts/diag/count_ns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+
+const pool = new Pool({ connectionString: url });
+
+async function main() {
+  const ns = process.env.NS || "rebecca/army/refs";
+  const like = ns.endsWith("%") ? ns : `${ns}%`;
+  const sql = `
+    select namespace, count(*) as cnt
+    from memories
+    where namespace like $1
+    group by 1
+    order by cnt desc
+    limit 50
+  `;
+  const r = await pool.query(sql, [like]);
+  console.table(r.rows);
+  await pool.end();
+}
+
+main().catch((e) => {
+  console.error("DB error:", e);
+  process.exit(1);
+});
diff --git a/apps/web/scripts/diag/list_columns.sh b/apps/web/scripts/diag/list_columns.sh
new file mode 100755
index 0000000..c6ee0a8
--- /dev/null
+++ b/apps/web/scripts/diag/list_columns.sh
@@ -0,0 +1,19 @@
+#!/usr/bin/env bash
+set -euo pipefail
+: "${DATABASE_URL:?set DATABASE_URL first}"
+
+TABLE="${1:-chunks}"
+
+psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off --quiet <<SQL
+SELECT
+  table_schema AS schema,
+  table_name   AS table,
+  ordinal_position AS pos,
+  column_name AS column,
+  data_type   AS type,
+  is_nullable AS nullable,
+  column_default
+FROM information_schema.columns
+WHERE table_schema='public' AND table_name='${TABLE}'
+ORDER BY ordinal_position;
+SQL
diff --git a/apps/web/scripts/diag/list_columns.sql b/apps/web/scripts/diag/list_columns.sql
new file mode 100644
index 0000000..47764da
--- /dev/null
+++ b/apps/web/scripts/diag/list_columns.sql
@@ -0,0 +1,15 @@
+-- Использование:
+--   psql "$DATABASE_URL" -v tbl='chunks' -f scripts/diag/list_columns.sql
+
+SELECT
+  c.table_schema AS schema,
+  c.table_name   AS table,
+  c.ordinal_position AS pos,
+  c.column_name  AS column,
+  c.data_type    AS type,
+  c.is_nullable  AS nullable,
+  c.column_default
+FROM information_schema.columns c
+WHERE c.table_schema = 'public'
+  AND c.table_name   = :'tbl'
+ORDER BY c.ordinal_position;
diff --git a/apps/web/scripts/diag/list_columns.ts b/apps/web/scripts/diag/list_columns.ts
new file mode 100644
index 0000000..d07fefa
--- /dev/null
+++ b/apps/web/scripts/diag/list_columns.ts
@@ -0,0 +1,27 @@
+// scripts/diag/list_columns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+
+const pool = new Pool({ connectionString: url });
+
+const CANDIDATE_TABLES = (process.env.CANDIDATE_TABLES || 'memories, memory, docs, documents, chunks, chunk, items')
+  .split(',')
+  .map(s => s.trim())
+  .filter(Boolean);
+
+async function main() {
+  const q = `
+    select table_name, column_name, data_type
+    from information_schema.columns
+    where table_schema='public' and table_name = any($1)
+    order by table_name, ordinal_position
+  `;
+  const r = await pool.query(q, [CANDIDATE_TABLES]);
+  console.table(r.rows);
+  await pool.end();
+}
+main().catch(e => { console.error("DB error:", e); process.exit(1); });
diff --git a/apps/web/scripts/diag/list_tables.sh b/apps/web/scripts/diag/list_tables.sh
new file mode 100755
index 0000000..6bb470d
--- /dev/null
+++ b/apps/web/scripts/diag/list_tables.sh
@@ -0,0 +1,13 @@
+#!/usr/bin/env bash
+set -euo pipefail
+: "${DATABASE_URL:?set DATABASE_URL first}"
+
+psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off <<'SQL'
+SELECT
+  schemaname AS schema,
+  tablename  AS table,
+  tableowner AS owner
+FROM pg_tables
+WHERE schemaname='public'
+ORDER BY tablename;
+SQL
diff --git a/apps/web/scripts/diag/list_tables.sql b/apps/web/scripts/diag/list_tables.sql
new file mode 100644
index 0000000..cb81f8e
--- /dev/null
+++ b/apps/web/scripts/diag/list_tables.sql
@@ -0,0 +1,8 @@
+-- Показать таблицы в public
+SELECT
+  schemaname  AS schema,
+  tablename   AS table,
+  tableowner  AS owner
+FROM pg_tables
+WHERE schemaname = 'public'
+ORDER BY 1,2;
diff --git a/apps/web/scripts/diag/slots_by_ns.ts b/apps/web/scripts/diag/slots_by_ns.ts
new file mode 100644
index 0000000..15b5437
--- /dev/null
+++ b/apps/web/scripts/diag/slots_by_ns.ts
@@ -0,0 +1,24 @@
+// scripts/diag/slots_by_ns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+const pool = new Pool({ connectionString: url });
+
+async function main() {
+  const ns = process.env.NS || "rebecca/army/refs";
+  const like = ns.endsWith("%") ? ns : `${ns}%`;
+  const sql = `
+    select slot, count(*)::int as cnt
+    from chunks
+    where ns like $1
+    group by 1
+    order by cnt desc, slot asc
+  `;
+  const r = await pool.query(sql, [like]);
+  console.table(r.rows);
+  await pool.end();
+}
+main().catch(e => { console.error("DB error:", e); process.exit(1); });
diff --git a/apps/web/scripts/diag/top_ns.ts b/apps/web/scripts/diag/top_ns.ts
new file mode 100644
index 0000000..4b39201
--- /dev/null
+++ b/apps/web/scripts/diag/top_ns.ts
@@ -0,0 +1,31 @@
+// scripts/diag/top_ns.ts
+import { Pool } from "pg";
+
+const url =
+  process.env.DATABASE_URL ||
+  process.env.PGURL ||
+  `postgres://${process.env.PGUSER || "postgres"}:${process.env.PGPASSWORD || ""}@${process.env.PGHOST || "localhost"}:${process.env.PGPORT || 5432}/${process.env.PGDATABASE || "postgres"}`;
+
+const pool = new Pool({ connectionString: url });
+
+async function main() {
+  const limit = Number(process.env.LIMIT || 20);
+
+  const q1 = `
+    select 'memories' as tbl, ns, count(*)::int as cnt
+    from memories group by 2 order by cnt desc limit $1
+  `;
+  const q2 = `
+    select 'chunks' as tbl, ns, count(*)::int as cnt
+    from chunks group by 2 order by cnt desc limit $1
+  `;
+  const [m, c] = await Promise.all([pool.query(q1, [limit]), pool.query(q2, [limit])]);
+
+  console.log("\n=== Top ns in memories ===");
+  console.table(m.rows);
+  console.log("\n=== Top ns in chunks ===");
+  console.table(c.rows);
+
+  await pool.end();
+}
+main().catch(e => { console.error("DB error:", e); process.exit(1); });
diff --git a/apps/web/scripts/e2e/bootstrap_demo.sh b/apps/web/scripts/e2e/bootstrap_demo.sh
new file mode 100755
index 0000000..f814f66
--- /dev/null
+++ b/apps/web/scripts/e2e/bootstrap_demo.sh
@@ -0,0 +1,81 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+WEB_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
+cd "$WEB_DIR"
+
+: "${DATABASE_URL:?DATABASE_URL is required}"
+BASE="${BASE:-http://localhost:3000}"
+NS="${NS:-rebecca/army/refs}"
+SLOT="${SLOT:-staging}"
+X_ADMIN_KEY="${X_ADMIN_KEY:-${ADMIN:-}}"
+
+echo "WEB_DIR=$WEB_DIR"
+echo "BASE=$BASE"
+echo "NS=$NS SLOT=$SLOT"
+
+psql_np() { psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off "$@"; }
+show_or_err() { jq 'if .ok==true or .ok==null then . else {ok,stage,error} end'; }
+
+# опциональный заголовок x-admin-key
+HDR_AUTH=()
+if [[ -n "${X_ADMIN_KEY:-}" ]]; then
+  HDR_AUTH=(-H "x-admin-key: ${X_ADMIN_KEY}")
+fi
+
+echo ">> TRUNCATE chunks for ns=$NS slot=$SLOT"
+psql_np -v ns="$NS" -v slot="$SLOT" <<'SQL'
+DELETE FROM chunks WHERE ns = :'ns' AND slot = :'slot';
+SQL
+
+echo ">> ingest URL (MDN)"
+jq -n --arg ns "$NS" --arg slot "$SLOT" '
+{
+  ns: $ns,
+  slot: $slot,
+  urls: [
+    "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop",
+    "https://developer.mozilla.org/en-US/docs/Web/API/Console/table",
+    "https://developer.mozilla.org/en-US/docs/Web/API/console/time"
+  ],
+  chunk: { chars: 1200, overlap: 150 }
+}
+' | curl -sS -X POST "$BASE/api/ingest/url" \
+      -H 'content-type: application/json' "${HDR_AUTH[@]}" \
+      -d @- | show_or_err
+
+PDF_URL="${LOCAL_PDF:-https://arxiv.org/pdf/2402.19472.pdf}"
+echo ">> ingest PDF ($PDF_URL)"
+jq -n --arg ns "$NS" --arg slot "$SLOT" --arg url "$PDF_URL" '
+{
+  ns: $ns,
+  slot: $slot,
+  url: $url,
+  chunk: { chars: 1200, overlap: 150 }
+}
+' | curl -sS -X POST "$BASE/api/ingest/pdf" \
+      -H 'content-type: application/json' "${HDR_AUTH[@]}" \
+      -d @- | show_or_err
+
+echo ">> ingest GitHub (openai/openai-cookbook, .md, limit 10)"
+jq -n --arg ns "$NS" --arg slot "$SLOT" '
+{
+  ns: $ns,
+  slot: $slot,
+  owner: "openai",
+  repo: "openai-cookbook",
+  ref: "main",
+  includeExt: [".md"],
+  cursor: 0,
+  limit: 10,
+  chunk: { chars: 1200, overlap: 150 }
+}
+' | curl -sS -X POST "$BASE/api/ingest/github" \
+      -H 'content-type: application/json' "${HDR_AUTH[@]}" \
+      -d @- | show_or_err
+
+echo ">> ANALYZE chunks"
+psql_np -c "ANALYZE chunks;"
+
+echo ">> summary:"
+psql_np -v ns="$NS" -f "$WEB_DIR/scripts/diag/count_ns.sql"
diff --git a/apps/web/scripts/evals/golden/rebecca_army_refs.jsonl b/apps/web/scripts/evals/golden/rebecca_army_refs.jsonl
new file mode 100644
index 0000000..ce84c55
--- /dev/null
+++ b/apps/web/scripts/evals/golden/rebecca_army_refs.jsonl
@@ -0,0 +1,8 @@
+{ "q": "Что такое микрозадачи (microtasks) в Event Loop и как они связаны с Promise?", "must_have": ["developer.mozilla.org", "html.spec.whatwg.org"], "k": 10 }
+{ "q": "Как работает console.time()/console.timeEnd() и есть ли ограничения по label?", "must_have": ["developer.mozilla.org"], "k": 10 }
+{ "q": "Как создать инициализационный агент GitHub Actions, который генерирует issue по шаблону?", "must_have": ["github.com", "docs.github.com"], "k": 10 }
+{ "q": "Какие ключевые отличия между queueMicrotask и process.nextTick?", "must_have": ["developer.mozilla.org", "nodejs.org"], "k": 10 }
+{ "q": "Что делает Console API метод console.groupCollapsed и как закрыть группу?", "must_have": ["developer.mozilla.org"], "k": 10 }
+{ "q": "Шаблон агента для веб-скрейпинга с ограничением домена: best practices?", "must_have": ["github.com", "chromedevtools.github.io"], "k": 10 }
+{ "q": "Порядок обработки задач: timers, pending, idle, poll, check, close — объясни на примере?", "must_have": ["nodejs.org"], "k": 10 }
+{ "q": "Есть ли у console.assert побочные эффекты и как правильно формировать условие?", "must_have": ["developer.mozilla.org"], "k": 10 }
diff --git a/apps/web/scripts/evals/run_eval.mjs b/apps/web/scripts/evals/run_eval.mjs
new file mode 100644
index 0000000..796c75a
--- /dev/null
+++ b/apps/web/scripts/evals/run_eval.mjs
@@ -0,0 +1,133 @@
+// Node >=18 (встроенный fetch). Читает JSONL кейсы, бьет /api/retrieve, считает hit@k и MRR.
+// BASE берется из env или дефолт http://localhost:3000
+import fs from "node:fs/promises";
+import path from "node:path";
+import { fileURLToPath } from "node:url";
+
+const __dirname = path.dirname(fileURLToPath(import.meta.url));
+
+const BASE = process.env.BASE?.trim() || "http://localhost:3000";
+const CASES = process.env.CASES?.trim() ||
+  path.join(__dirname, "sample_cases.jsonl");
+const OUT_MD = process.env.OUT_MD?.trim() ||
+  path.resolve(__dirname, "../../../docs/evals/latest.md");
+
+// маленький помощник: читать JSONL
+async function readJsonl(file) {
+  const txt = await fs.readFile(file, "utf8");
+  return txt.split(/\r?\n/).filter(Boolean).map((line, i) => {
+    try { return JSON.parse(line); } catch (e) {
+      throw new Error(`JSONL parse error at line ${i+1}: ${e.message}`);
+    }
+  });
+}
+
+// одно обращение к /api/retrieve
+async function retrieve({ q, ns, slot, k = 5, candidateK = 500 }) {
+  const body = {
+    q, ns, slot,
+    topK: k, candidateK,
+    minSimilarity: 0.0,
+    nsMode: "prefix",
+  };
+  const r = await fetch(`${BASE}/api/retrieve`, {
+    method: "POST",
+    headers: { "content-type": "application/json" },
+    body: JSON.stringify(body),
+  });
+  if (!r.ok) {
+    const t = await r.text().catch(()=> "");
+    throw new Error(`retrieve ${r.status} ${r.statusText}: ${t.slice(0,200)}`);
+  }
+  return await r.json();
+}
+
+// проверяем «успех» кейса
+function judge(caseItem, items) {
+  const k = caseItem.k ?? 5;
+  const urls = items.map(x => x.url || "");
+  // критерии: anyOfDomains ИЛИ substring ИЛИ регэкспы (любой, если указан)
+  const anyOfDomains = Array.isArray(caseItem.anyOfDomains) ? caseItem.anyOfDomains : [];
+  const substr = Array.isArray(caseItem.mustContain) ? caseItem.mustContain : [];
+  const regexes = Array.isArray(caseItem.mustMatch) ? caseItem.mustMatch.map(s => new RegExp(s)) : [];
+
+  // функция попадания для одного url
+  const okUrl = (u) => {
+    if (!u) return false;
+    if (anyOfDomains.length) {
+      try {
+        const h = new URL(u).hostname.toLowerCase();
+        if (anyOfDomains.some(d => h === d || h.endsWith(`.${d}`))) return true;
+      } catch{}
+    }
+    if (substr.length && substr.some(s => u.includes(s))) return true;
+    if (regexes.length && regexes.some(rx => rx.test(u))) return true;
+    return false;
+  };
+
+  // hit@k
+  const topK = urls.slice(0, k);
+  const hit = topK.some(okUrl) ? 1 : 0;
+
+  // MRR (reciprocal rank первого совпадения)
+  let rr = 0;
+  for (let i = 0; i < topK.length; i++) {
+    if (okUrl(topK[i])) { rr = 1 / (i + 1); break; }
+  }
+
+  // индекс первого совпадения (для отчета)
+  let firstIdx = -1;
+  for (let i = 0; i < topK.length; i++) {
+    if (okUrl(topK[i])) { firstIdx = i; break; }
+  }
+
+  return { hit, rr, firstIdx, topKUrls: topK };
+}
+
+async function main() {
+  const cases = await readJsonl(CASES);
+  if (!cases.length) throw new Error("no cases in JSONL");
+
+  const results = [];
+  for (const c of cases) {
+    const ns = c.ns || process.env.NS;
+    const slot = c.slot || process.env.SLOT || "staging";
+    if (!ns) throw new Error("case missing ns (or export NS=...)");
+
+    const res = await retrieve({ q: c.q, ns, slot, k: c.k ?? 5, candidateK: c.candidateK ?? 500 });
+    const judgeRes = judge(c, res.items || []);
+    results.push({ case: c, judge: judgeRes });
+  }
+
+  const n = results.length;
+  const hitAtK = results.reduce((s, r) => s + r.judge.hit, 0) / n;
+  const mrr   = results.reduce((s, r) => s + r.judge.rr, 0) / n;
+
+  // печать в консоль (кратко)
+  console.log(JSON.stringify({
+    n, hitAtK: Number(hitAtK.toFixed(4)), mrr: Number(mrr.toFixed(4))
+  }, null, 2));
+
+  // отчёт в Markdown
+  const lines = [];
+  lines.push(`# Retrieval Eval (RC v1)`);
+  lines.push(`Base: \`${BASE}\``);
+  lines.push(`Date: ${new Date().toISOString()}`);
+  lines.push(``);
+  lines.push(`**Cases:** ${n}`);
+  lines.push(`**hit@k:** ${hitAtK.toFixed(4)}`);
+  lines.push(`**MRR:** ${mrr.toFixed(4)}`);
+  lines.push(``);
+  lines.push(`| # | q | k | hit | rr | first_idx | topK urls |`);
+  lines.push(`|---|---|---:|---:|---:|---:|---|`);
+  results.forEach((r, i) => {
+    const { q, k } = { q: r.case.q, k: r.case.k ?? 5 };
+    const { hit, rr, firstIdx, topKUrls } = r.judge;
+    lines.push(`| ${i+1} | ${q.replace(/\|/g,"\\|")} | ${k} | ${hit} | ${rr.toFixed(3)} | ${firstIdx} | ${topKUrls.map(u=>u||"-").join("<br>")} |`);
+  });
+  lines.push(``);
+  await fs.mkdir(path.dirname(OUT_MD), { recursive: true });
+  await fs.writeFile(OUT_MD, lines.join("\n"), "utf8");
+}
+
+main().catch(err => { console.error(err); process.exit(1); });
diff --git a/apps/web/scripts/evals/run_eval.ts b/apps/web/scripts/evals/run_eval.ts
new file mode 100644
index 0000000..28b4754
--- /dev/null
+++ b/apps/web/scripts/evals/run_eval.ts
@@ -0,0 +1,144 @@
+// apps/web/scripts/evals/run_eval.ts
+import fs from "node:fs/promises";
+import path from "node:path";
+
+type Case = {
+  q: string;
+  // подсказка, как мы считаем "попадание"
+  must_url_substr?: string;
+
+  // per-case опции запроса
+  topK?: number;
+  candidateK?: number;
+  minSimilarity?: number;
+  nsMode?: "strict" | "prefix";
+  domainAllow?: string[];      // НОВОЕ: allow-домены для этого кейса
+  domainDeny?: string[];       // опционально
+};
+
+type RetrieveReq = {
+  q: string;
+  ns: string;
+  slot: string;
+  topK: number;
+  candidateK: number;
+  minSimilarity: number;
+  nsMode: "strict" | "prefix";
+  domainFilter?: { allow?: string[]; deny?: string[] };
+};
+
+const BASE = process.env.BASE || "http://localhost:3000";
+const NS   = process.env.NS   || "rebecca/army/refs";
+const SLOT = process.env.SLOT || "staging";
+const CASES = process.env.EVAL_CASES || "apps/web/scripts/evals/sample_cases.jsonl";
+const OUTMD = process.env.EVAL_REPORT || "apps/web/docs/evals/latest.md";
+
+async function readJsonl(file: string): Promise<Case[]> {
+  const abs = path.resolve(file);
+  const raw = await fs.readFile(abs, "utf8");
+  return raw
+    .split(/\r?\n/)
+    .map(l => l.trim())
+    .filter(Boolean)
+    .map(l => JSON.parse(l));
+}
+
+async function callRetrieve(req: RetrieveReq) {
+  const res = await fetch(`${BASE}/api/retrieve`, {
+    method: "POST",
+    headers: { "content-type": "application/json" },
+    body: JSON.stringify(req),
+  });
+  const j = await res.json();
+  if (!res.ok) {
+    throw new Error(`retrieve HTTP ${res.status}: ${JSON.stringify(j)}`);
+  }
+  return j;
+}
+
+function metricHitAtK(ranks: number[]): number {
+  // ranks: 1..K для попавших; 0 — промах
+  const hits = ranks.filter(x => x > 0).length;
+  return ranks.length ? hits / ranks.length : 0;
+}
+
+function metricMRR(ranks: number[]): number {
+  let s = 0;
+  for (const r of ranks) s += r > 0 ? 1 / r : 0;
+  return ranks.length ? s / ranks.length : 0;
+}
+
+function mdReport(now: Date, ranks: number[], rows: Array<{q:string; rank:number; url?:string; urls:string[];}>) {
+  const hitAtK = metricHitAtK(ranks);
+  const mrr    = metricMRR(ranks);
+  const lines: string[] = [];
+  lines.push(`# Eval report`);
+  lines.push(`Date: ${now.toISOString()}`);
+  lines.push(`NS: \`${NS}\`, slot: \`${SLOT}\`, base: \`${BASE}\``);
+  lines.push("");
+  lines.push(`**Summary:** n=${ranks.length}, hit@k=${hitAtK.toFixed(3)}, MRR=${mrr.toFixed(3)}`);
+  lines.push("");
+  lines.push(`| # | query | rank | top URLs |`);
+  lines.push(`|--:|-------|-----:|----------|`);
+  rows.forEach((r, i) => {
+    const show = r.urls.slice(0, 3).map(u => u.replace(/^https?:\/\//, "")).join("<br>");
+    lines.push(`| ${i+1} | ${r.q} | ${r.rank || 0} | ${show} |`);
+  });
+  lines.push("");
+  return lines.join("\n");
+}
+
+async function main() {
+  const cases = await readJsonl(CASES);
+  const ranks: number[] = [];
+  const rowsForMd: Array<{q:string; rank:number; url?:string; urls:string[];}> = [];
+
+  for (const c of cases) {
+    const req: RetrieveReq = {
+      q: c.q,
+      ns: NS,
+      slot: SLOT,
+      topK: c.topK ?? 5,
+      candidateK: c.candidateK ?? Math.max(200, c.topK ?? 5),
+      minSimilarity: c.minSimilarity ?? 0.0,
+      nsMode: c.nsMode ?? "prefix",
+    };
+    if ((c.domainAllow && c.domainAllow.length) || (c.domainDeny && c.domainDeny.length)) {
+      req.domainFilter = {};
+      if (c.domainAllow?.length) req.domainFilter.allow = c.domainAllow;
+      if (c.domainDeny?.length)  req.domainFilter.deny  = c.domainDeny;
+    }
+
+    const r = await callRetrieve(req);
+    const items: Array<{url?: string}> = Array.isArray(r?.items) ? r.items : [];
+    const urls = items.map(it => it?.url || "");
+
+    // ранк первой ссылки, содержащей must_url_substr (если задан)
+    let rank = 0;
+    if (c.must_url_substr) {
+      const needle = c.must_url_substr.toLowerCase();
+      const idx = urls.findIndex(u => (u || "").toLowerCase().includes(needle));
+      rank = idx >= 0 ? idx + 1 : 0;
+    } else {
+      // если критерий не задан — считаем попаданием, если вообще есть хоть одна ссылка
+      rank = urls.length > 0 ? 1 : 0;
+    }
+
+    ranks.push(rank);
+    rowsForMd.push({ q: c.q, rank, url: urls[rank-1], urls });
+  }
+
+  const now = new Date();
+  const out = { n: ranks.length, hitAtK: metricHitAtK(ranks), mrr: metricMRR(ranks) };
+  console.log(JSON.stringify(out, null, 2));
+
+  // markdown отчёт
+  const md = mdReport(now, ranks, rowsForMd);
+  await fs.mkdir(path.dirname(OUTMD), { recursive: true });
+  await fs.writeFile(OUTMD, md, "utf8");
+}
+
+main().catch(err => {
+  console.error(err);
+  process.exit(1);
+});
diff --git a/apps/web/scripts/evals/sample_cases.jsonl b/apps/web/scripts/evals/sample_cases.jsonl
new file mode 100644
index 0000000..3ec39db
--- /dev/null
+++ b/apps/web/scripts/evals/sample_cases.jsonl
@@ -0,0 +1,5 @@
+{"q":"event loop microtask","must_url_substr":"developer.mozilla.org/en-US/docs/Web/JavaScript/Event_loop","domainAllow":["developer.mozilla.org"]}
+{"q":"console.time usage","must_url_substr":"developer.mozilla.org/en-US/docs/Web/API/console/time","domainAllow":["developer.mozilla.org"]}
+{"q":"console.table example","must_url_substr":"developer.mozilla.org/en-US/docs/Web/API/Console/table","domainAllow":["developer.mozilla.org"]}
+{"q":"openai cookbook examples","must_url_substr":"github.com/openai/openai-cookbook","domainAllow":["github.com"]}
+{"q":"Mastering AI Agents","must_url_substr":"file://"}
diff --git a/apps/web/scripts/ingest_github_paged.sh b/apps/web/scripts/ingest_github_paged.sh
new file mode 100755
index 0000000..e9d189b
--- /dev/null
+++ b/apps/web/scripts/ingest_github_paged.sh
@@ -0,0 +1,130 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Defaults
+APP="${APP:-$HOME/projects/freya-rebecca/apps/web}"
+BASE="${BASE:-http://localhost:3000}"
+NS="${NS:-rebecca/army/refs}"
+SLOT="${SLOT:-staging}"
+OWNER=""
+REPO=""
+REF="main"
+PATH_PREFIX=""
+INCLUDE_LIST=""   # e.g. ".md,.mdx,.py,.ipynb,.txt"
+EXCLUDE_LIST=""   # optional
+LIMIT=250
+CURSOR=0
+DRYRUN="false"
+
+usage() {
+  cat <<EOF
+Usage:
+  $(basename "$0") --ns NS --slot SLOT --owner OWNER --repo REPO [--ref main] [--path subdir]
+                   [--include ".md,.mdx,.py,.ipynb,.txt"] [--exclude ".png,.pdf"]
+                   [--limit 250] [--cursor 0] [--dry-run]
+
+Env:
+  APP (default: $APP)
+  BASE (default: $BASE)
+  ADM  (x-admin-key header; auto-read from \$APP/.env.local if пусто)
+
+Скрипт шагает по страницам (cursor -> nextCursor) до конца.
+EOF
+}
+
+# Parse args
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --ns) NS="$2"; shift 2;;
+    --slot) SLOT="$2"; shift 2;;
+    --owner) OWNER="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --ref) REF="$2"; shift 2;;
+    --path) PATH_PREFIX="$2"; shift 2;;
+    --include) INCLUDE_LIST="$2"; shift 2;;
+    --exclude) EXCLUDE_LIST="$2"; shift 2;;
+    --limit) LIMIT="$2"; shift 2;;
+    --cursor) CURSOR="$2"; shift 2;;
+    --dry-run) DRYRUN="true"; shift 1;;
+    -h|--help) usage; exit 0;;
+    *) echo "Unknown arg: $1"; usage; exit 1;;
+  end esac
+done
+
+# Admin header
+if [[ -z "${ADM:-}" ]]; then
+  if [[ -f "$APP/.env.local" ]]; then
+    ADM="x-admin-key: $(grep -E '^X_ADMIN_KEY=' "$APP/.env.local" | cut -d= -f2- || true)"
+  fi
+fi
+if [[ -z "$ADM" ]]; then
+  echo "!! ADM (x-admin-key) не найден. Укажи переменную окружения ADM или положи X_ADMIN_KEY в $APP/.env.local"
+  exit 1
+fi
+
+# Build JSON arrays for include/exclude safely
+include_json="null"
+exclude_json="null"
+if [[ -n "$INCLUDE_LIST" ]]; then
+  include_json=$(jq -nc --arg s "$INCLUDE_LIST" '$s | split(",")')
+fi
+if [[ -n "$EXCLUDE_LIST" ]]; then
+  exclude_json=$(jq -nc --arg s "$EXCLUDE_LIST" '$s | split(",")')
+fi
+
+echo ">>> Ingest: ns=$NS slot=$SLOT owner=$OWNER repo=$REPO ref=$REF limit=$LIMIT from cursor=$CURSOR dryRun=$DRYRUN"
+echo "    BASE=$BASE APP=$APP"
+echo "    include=$INCLUDE_LIST exclude=$EXCLUDE_LIST"
+echo
+
+next="$CURSOR"
+total_written=0
+total_chunks=0
+page=0
+
+while :; do
+  # Compose payload with jq to avoid quoting bugs
+  payload=$(jq -nc \
+    --arg ns "$NS" \
+    --arg slot "$SLOT" \
+    --arg owner "$OWNER" \
+    --arg repo "$REPO" \
+    --arg ref "$REF" \
+    --arg path "$PATH_PREFIX" \
+    --argjson include "$include_json" \
+    --argjson exclude "$exclude_json" \
+    --argjson cursor "$next" \
+    --argjson limit "$LIMIT" \
+    --argjson dryRun "$DRYRUN" \
+    '{
+      ns:$ns, slot:$slot, owner:$owner, repo:$repo,
+      ref:$ref, path:$path,
+      includeExt:$include, excludeExt:$exclude,
+      cursor:$cursor, limit:$limit, dryRun:$dryRun
+    }')
+
+  resp=$(curl -sS -X POST "$BASE/api/ingest/github" \
+    -H 'Content-Type: application/json' -H "$ADM" \
+    --data-binary "$payload")
+
+  ok=$(echo "$resp" | jq -r '.ok // false')
+  if [[ "$ok" != "true" ]]; then
+    echo "$resp" | jq .
+    echo "!! Ошибка, прерываем."
+    exit 1
+  fi
+
+  page=$((page+1))
+  echo ">>> Page #$page"
+  echo "$resp" | jq '{totalFiles,windowStart,windowEnd,pageFiles,chunks,written,nextCursor,ms}'
+  written=$(echo "$resp" | jq -r '.written // 0')
+  chunks=$(echo "$resp" | jq -r '.chunks // 0')
+  total_written=$((total_written + written))
+  total_chunks=$((total_chunks + chunks))
+
+  next=$(echo "$resp" | jq -r '.nextCursor')
+  if [[ "$next" == "null" || -z "$next" ]]; then
+    echo ">>> Done. Total pages=$page, chunks=$total_chunks, written=$total_written"
+    exit 0
+  fi
+done
diff --git a/apps/web/scripts/ingest_readme_pdfs.sh b/apps/web/scripts/ingest_readme_pdfs.sh
new file mode 100755
index 0000000..57422eb
--- /dev/null
+++ b/apps/web/scripts/ingest_readme_pdfs.sh
@@ -0,0 +1,59 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+APP="${APP:-$HOME/projects/freya-rebecca/apps/web}"
+BASE="${BASE:-http://localhost:3000}"
+NS="${NS:-rebecca/army/refs}"
+SLOT="${SLOT:-staging}"
+OWNER="${OWNER:?-- set OWNER}"
+REPO="${REPO:?-- set REPO}"
+REF="${REF:-main}"
+LIMIT="${LIMIT:-30}"              # сколько URL отправлять за 1 запрос
+MAX_FILE_BYTES="${MAX_FILE_BYTES:-20000000}"
+
+# заголовок admin
+if [[ -z "${ADM:-}" ]]; then
+  if [[ -f "$APP/.env.local" ]]; then
+    ADM="x-admin-key: $(grep -E '^X_ADMIN_KEY=' "$APP/.env.local" | cut -d= -f2- || true)"
+  fi
+fi
+[[ -z "${ADM:-}" ]] && { echo "!! ADM not set"; exit 1; }
+
+echo ">>> Read README for $OWNER/$REPO@$REF ..."
+readme_json="$(curl -fsS "https://api.github.com/repos/$OWNER/$REPO/contents/README.md?ref=$REF")"
+content_b64="$(jq -r '.content // empty' <<<"$readme_json")"
+[[ -z "$content_b64" ]] && { echo "No README.md content"; exit 0; }
+readme_txt="$(printf '%s' "$content_b64" | tr -d '\n' | base64 -d)"
+
+# добываем все http(s) ссылки и фильтруем pdf
+mapfile -t pdfs < <(printf '%s\n' "$readme_txt" | \
+  grep -oE '(https?://[^ )>\"]+)' | \
+  sed 's/[),.;:]*$//' | \
+  grep -iE '\.pdf($|\?)' | sort -u)
+
+echo "found PDF links: ${#pdfs[@]}"
+((${#pdfs[@]}==0)) && exit 0
+
+# пачками шлём в /api/ingest/url
+cursor=0
+while (( cursor < ${#pdfs[@]} )); do
+  batch=( "${pdfs[@]:$cursor:$LIMIT}" )
+  cursor=$((cursor + ${#batch[@]} ))
+
+  printf '>>> Batch of %d\n' "${#batch[@]}"
+
+  # собираем JSON-массив
+  urls_json="$(printf '%s\n' "${batch[@]}" | jq -R . | jq -s .)"
+
+  curl -fsS -X POST "$BASE/api/ingest/url" \
+    -H 'Content-Type: application/json' -H "$ADM" \
+    --data-binary "$(jq -n \
+      --arg ns "$NS" \
+      --arg slot "$SLOT" \
+      --argjson urls "$urls_json" \
+      --argjson maxFileBytes "$MAX_FILE_BYTES" \
+      '{ns:$ns,slot:$slot,urls:$urls,maxFileBytes:$maxFileBytes}')"
+  echo
+done
+
+echo ">>> Done."
diff --git a/apps/web/scripts/migrate-g0.js b/apps/web/scripts/migrate-g0.js
old mode 100644
new mode 100755
diff --git a/apps/web/scripts/migrate.sh b/apps/web/scripts/migrate.sh
new file mode 100755
index 0000000..5b0ecdf
--- /dev/null
+++ b/apps/web/scripts/migrate.sh
@@ -0,0 +1,37 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Абсолютные пути к scripts/ и scripts/migrations/
+SCRIPTS_DIR="$(cd "$(dirname "$0")" && pwd)"
+MIG_DIR="${SCRIPTS_DIR}/migrations"
+
+# Проверим переменные и psql
+: "${DATABASE_URL:?DATABASE_URL is not set (export it or source apps/web/.env.local)}"
+command -v psql >/dev/null || { echo "psql not found"; exit 1; }
+
+echo "== running migrations from: ${MIG_DIR}"
+
+# Базовые расширения (идемпотентно)
+psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off <<'SQL'
+CREATE EXTENSION IF NOT EXISTS vector;
+CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
+CREATE EXTENSION IF NOT EXISTS pgcrypto;
+SQL
+
+# Прогоним все G*.sql по алфавиту (G0_init.sql, G5_*.sql, и т.д.)
+shopt -s nullglob
+for f in "${MIG_DIR}"/G*.sql; do
+  echo "-- psql -f $(basename "$f")"
+  psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off -f "$f"
+done
+shopt -u nullglob
+
+# (опционально) поставить дефолт для ivfflat.probes
+if [[ -n "${RETRIEVE_PROBES:-}" ]]; then
+  DBNAME="$(psql "$DATABASE_URL" -Atc 'select current_database()')"
+  psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -P pager=off -c \
+    "ALTER DATABASE \"${DBNAME}\" SET ivfflat.probes = ${RETRIEVE_PROBES};"
+  echo "ivfflat.probes default -> ${RETRIEVE_PROBES}"
+fi
+
+echo "Migrations OK."
diff --git a/apps/web/scripts/migrations/G0_init.sql b/apps/web/scripts/migrations/G0_init.sql
new file mode 100644
index 0000000..d5ba0c2
--- /dev/null
+++ b/apps/web/scripts/migrations/G0_init.sql
@@ -0,0 +1,24 @@
+-- Схема и расширения (идемпотентно)
+CREATE EXTENSION IF NOT EXISTS vector;
+CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
+CREATE EXTENSION IF NOT EXISTS pgcrypto;
+
+CREATE TABLE IF NOT EXISTS public.chunks (
+  id            uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
+  ns            text NOT NULL,
+  slot          text NOT NULL CHECK (slot IN ('staging','prod')),
+  content       text NOT NULL,
+  embedding     vector(1536) NOT NULL,
+  url           text,
+  title         text,
+  snippet       text,
+  published_at  timestamptz,
+  source_type   text,
+  kind          text,
+  metadata      jsonb NOT NULL DEFAULT '{}'::jsonb,
+  content_hash  text NOT NULL,
+  source_id     text,
+  chunk_no      integer NOT NULL,
+  created_at    timestamptz NOT NULL DEFAULT now(),
+  updated_at    timestamptz NOT NULL DEFAULT now()
+);
diff --git a/apps/web/scripts/migrations/G5_indexes_upserts.sql b/apps/web/scripts/migrations/G5_indexes_upserts.sql
new file mode 100644
index 0000000..9d6abc5
--- /dev/null
+++ b/apps/web/scripts/migrations/G5_indexes_upserts.sql
@@ -0,0 +1,17 @@
+-- Индексы и инварианты (идемпотентно)
+CREATE INDEX IF NOT EXISTS chunks_ns_slot_idx
+  ON public.chunks(ns, slot);
+
+CREATE INDEX IF NOT EXISTS chunks_ns_slot_published_idx
+  ON public.chunks(ns, slot, published_at DESC);
+
+CREATE INDEX IF NOT EXISTS chunks_metadata_gin_idx
+  ON public.chunks USING gin(metadata);
+
+-- ANN индекс по эмбеддингам
+CREATE INDEX IF NOT EXISTS chunks_embedding_ivfflat_idx
+  ON public.chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
+
+-- Ключ идемпотентности ingest
+CREATE UNIQUE INDEX IF NOT EXISTS chunks_unique_source_idx
+  ON public.chunks(ns, slot, COALESCE(source_id, ''), chunk_no);
diff --git a/apps/web/scripts/migrations/g5_backfill_chunks_from_memories.sql b/apps/web/scripts/migrations/g5_backfill_chunks_from_memories.sql
new file mode 100644
index 0000000..de53fb6
--- /dev/null
+++ b/apps/web/scripts/migrations/g5_backfill_chunks_from_memories.sql
@@ -0,0 +1,19 @@
+-- Бэκфилл chunks из memories по rebecca/army/refs*, без дублей по id
+
+-- 1) Индекс по ns (ускорит фильтрацию/вставку)
+CREATE INDEX IF NOT EXISTS idx_memories_ns ON public.memories(ns);
+
+-- 2) Вставляем недостающие записи
+INSERT INTO public.chunks (id, kind, ns, slot, content, embedding, metadata, created_at)
+SELECT m.id, m.kind, m.ns, m.slot, m.content, m.embedding, m.metadata, m.created_at
+FROM public.memories m
+LEFT JOIN public.chunks c ON c.id = m.id
+WHERE m.ns LIKE 'rebecca/army/refs%'
+  AND c.id IS NULL;
+
+-- 3) Индексы на chunks
+CREATE INDEX IF NOT EXISTS idx_chunks_ns ON public.chunks(ns);
+
+-- Если у тебя pgvector установлен и тип embedding = vector(...), раскомментируй строку ниже.
+-- Подбери операторный класс под твою метрику: vector_cosine_ops / vector_l2_ops / vector_ip_ops
+-- CREATE INDEX IF NOT EXISTS idx_chunks_embedding ON public.chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
-- 
2.43.0

