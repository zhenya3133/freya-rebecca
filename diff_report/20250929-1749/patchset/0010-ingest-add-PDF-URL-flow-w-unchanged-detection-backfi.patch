From 7ac47ad0df5ba273424f01cc76a82faf7dff74ed Mon Sep 17 00:00:00 2001
From: zhenya3133 <verbenkoe@gmail.com>
Date: Sat, 27 Sep 2025 08:53:09 +0300
Subject: [PATCH 10/14] ingest: add PDF+URL flow w/ unchanged detection &
 backfill; GitHub ingest w/ pagination, includeExt/path filters,
 dryRun/skipEmbeddings; helper scripts

---
 apps/web/scripts/embed_backfill.sh          |  40 +++
 apps/web/scripts/ingest_github.sh           |  83 +++++
 apps/web/scripts/ingest_pdf.sh              |  63 ++++
 apps/web/scripts/ingest_pdf_file.sh         |  66 ++++
 apps/web/scripts/ingest_url.sh              |  21 ++
 apps/web/scripts/retrieve.sh                |  98 ++++++
 apps/web/scripts/smoke_pdf.sh               |  49 +++
 apps/web/src/app/api/ingest/github/route.ts | 245 +++++++++------
 apps/web/src/app/api/ingest/pdf/route.ts    |  89 ++++--
 apps/web/src/lib/admin.ts                   |  21 ++
 apps/web/src/lib/ingest_upsert.ts           | 324 +++++++++-----------
 scripts/ingest_github_paged.sh              |  38 ++-
 12 files changed, 841 insertions(+), 296 deletions(-)
 create mode 100755 apps/web/scripts/embed_backfill.sh
 create mode 100755 apps/web/scripts/ingest_github.sh
 create mode 100755 apps/web/scripts/ingest_pdf.sh
 create mode 100755 apps/web/scripts/ingest_pdf_file.sh
 create mode 100755 apps/web/scripts/ingest_url.sh
 create mode 100755 apps/web/scripts/retrieve.sh
 create mode 100755 apps/web/scripts/smoke_pdf.sh
 create mode 100644 apps/web/src/lib/admin.ts
 mode change 100644 => 100755 scripts/ingest_github_paged.sh

diff --git a/apps/web/scripts/embed_backfill.sh b/apps/web/scripts/embed_backfill.sh
new file mode 100755
index 0000000..cb75542
--- /dev/null
+++ b/apps/web/scripts/embed_backfill.sh
@@ -0,0 +1,40 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Использование:
+#   ./scripts/embed_backfill.sh rebecca/army/refs staging 300 16
+#   ./scripts/embed_backfill.sh "" "" 300 16        # без фильтров ns/slot
+#
+# Требования: BASE, ADMIN_KEY в окружении.
+
+: "${BASE:?need BASE like http://localhost:3000}"
+: "${ADMIN_KEY:?need ADMIN_KEY (dev-12345)}"
+
+NS="${1:-}"
+SLOT="${2:-}"
+LIMIT="${3:-200}"
+BATCH="${4:-16}"
+
+if [[ -n "$NS" && -n "$SLOT" ]]; then
+  jq -n \
+    --arg ns "$NS" \
+    --arg slot "$SLOT" \
+    --argjson limit "$LIMIT" \
+    --argjson batchSize "$BATCH" \
+    '{ ns:$ns, slot:$slot, limit:$limit, batchSize:$batchSize }' \
+  | curl -sS -X POST "$BASE/api/admin/embed-backfill" \
+      -H "content-type: application/json" \
+      -H "x-admin-key: $ADMIN_KEY" \
+      --data-binary @-
+else
+  jq -n \
+    --argjson limit "$LIMIT" \
+    --argjson batchSize "$BATCH" \
+    '{ limit:$limit, batchSize:$batchSize }' \
+  | curl -sS -X POST "$BASE/api/admin/embed-backfill" \
+      -H "content-type: application/json" \
+      -H "x-admin-key: $ADMIN_KEY" \
+      --data-binary @-
+fi
+
+echo
\ No newline at end of file
diff --git a/apps/web/scripts/ingest_github.sh b/apps/web/scripts/ingest_github.sh
new file mode 100755
index 0000000..1b58ab2
--- /dev/null
+++ b/apps/web/scripts/ingest_github.sh
@@ -0,0 +1,83 @@
+# apps/web/scripts/ingest_github.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+BASE="${BASE:-http://localhost:3000}"
+ADMIN_KEY="${ADMIN_KEY:-${X_ADMIN_KEY:-}}"
+
+if [ $# -lt 3 ]; then
+  echo "Usage: $0 <ns> <slot> <owner/repo> [--ref <ref>] [--path <prefix>] [--include \".md,.txt\"] [--exclude \".png,.jpg\"] [--cursor N] [--limit M] [--dry] [--no-emb] [--chars N] [--overlap N]" >&2
+  exit 1
+fi
+
+ns="$1"; slot="$2"; or="$3"; shift 3
+owner="${or%%/*}"; repo="${or#*/}"
+
+ref="main"
+path=""
+include=""
+exclude=""
+cursor=""
+limit=""
+dry="false"
+noemb="false"
+chars=""
+overlap=""
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --ref) ref="$2"; shift 2;;
+    --path) path="$2"; shift 2;;
+    --include) include="$2"; shift 2;;    # строка через запятую: ".md,.txt"
+    --exclude) exclude="$2"; shift 2;;
+    --cursor) cursor="$2"; shift 2;;
+    --limit) limit="$2"; shift 2;;
+    --dry) dry="true"; shift 1;;
+    --no-emb) noemb="true"; shift 1;;
+    --chars) chars="$2"; shift 2;;
+    --overlap) overlap="$2"; shift 2;;
+    *) echo "Unknown arg: $1" >&2; exit 1;;
+  esac
+done
+
+jq -n \
+  --arg ns "$ns" \
+  --arg slot "$slot" \
+  --arg owner "$owner" \
+  --arg repo "$repo" \
+  --arg ref "$ref" \
+  --arg path "$path" \
+  --arg include "$include" \
+  --arg exclude "$exclude" \
+  --arg cursor "$cursor" \
+  --arg limit "$limit" \
+  --arg dry "$dry" \
+  --arg noemb "$noemb" \
+  --arg chars "$chars" \
+  --arg overlap "$overlap" '
+{
+  ns:$ns, slot:$slot,
+  owner:$owner, repo:$repo, ref:$ref,
+}
++ ( ($path|length)>0 ? { path:$path } : {} )
++ ( ($include|length)>0 ? { includeExt: ($include|split(",")|map(.|gsub("^\\s+|\\s+$";""))) } : {} )
++ ( ($exclude|length)>0 ? { excludeExt: ($exclude|split(",")|map(.|gsub("^\\s+|\\s+$";""))) } : {} )
++ ( ($cursor|length)>0 ? { cursor: ($cursor|tonumber) } : {} )
++ ( ($limit|length)>0 ? { limit: ($limit|tonumber) } : {} )
++ ( ($dry == "true") ? { dryRun:true } : {} )
++ ( ($noemb == "true") ? { skipEmbeddings:true } : {} )
++ (
+    ( ($chars|length)>0 or ($overlap|length)>0 )
+    ? { chunk:
+        ( {}
+          + ( ($chars|length)>0 ? { chars: ($chars|tonumber) } : {} )
+          + ( ($overlap|length)>0 ? { overlap: ($overlap|tonumber) } : {} )
+        )
+      }
+    : {}
+  )
+' \
+| curl -sS -X POST "$BASE/api/ingest/github" \
+    -H "content-type: application/json" \
+    -H "x-admin-key: ${ADMIN_KEY}" \
+    --data-binary @-
diff --git a/apps/web/scripts/ingest_pdf.sh b/apps/web/scripts/ingest_pdf.sh
new file mode 100755
index 0000000..a772412
--- /dev/null
+++ b/apps/web/scripts/ingest_pdf.sh
@@ -0,0 +1,63 @@
+# apps/web/scripts/ingest_pdf.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Использование:
+#   ./scripts/ingest_pdf.sh NS SLOT URL                 # обычная загрузка
+#   ./scripts/ingest_pdf.sh NS SLOT URL --dry           # dry-run
+#   ./scripts/ingest_pdf.sh NS SLOT URL --no-emb        # пропустить эмбеддинги
+#   ./scripts/ingest_pdf.sh NS SLOT URL --max 20000000  # лимит размера PDF (байты)
+#   ./scripts/ingest_pdf.sh NS SLOT URL --chars 1200 --overlap 180
+#
+# Требуется окружение: BASE, ADMIN_KEY
+
+: "${BASE:?need BASE like http://localhost:3000}"
+: "${ADMIN_KEY:?need ADMIN_KEY}"
+
+NS="${1:?need ns}"
+SLOT="${2:?need slot}"
+URL="${3:?need pdf url}"
+shift 3 || true
+
+DRY=false
+NOEMB=false
+MAXBYTES=0
+CHARS=0
+OVERLAP=0
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --dry) DRY=true ;;
+    --no-emb) NOEMB=true ;;
+    --max) shift; MAXBYTES="${1:?need number}" ;;
+    --chars) shift; CHARS="${1:?need number}" ;;
+    --overlap) shift; OVERLAP="${1:?need number}" ;;
+    *) echo "Unknown flag: $1" >&2; exit 2 ;;
+  esac
+  shift || true
+done
+
+make_chunk_json() {
+  local c="$1" o="$2"
+  jq -n --argjson c "${c:-0}" --argjson o "${o:-0}" '
+    (if $c>0 then {chars:$c} else {} end)
+    + (if $o>0 then {overlap:$o} else {} end)
+  '
+}
+
+CHUNK_JSON="$(make_chunk_json "$CHARS" "$OVERLAP")"
+
+jq -n \
+  --arg ns "$NS" --arg slot "$SLOT" --arg url "$URL" \
+  --argjson dry "$([[ "$DRY" == true ]] && echo true || echo false)" \
+  --argjson noemb "$([[ "$NOEMB" == true ]] && echo true || echo false)" \
+  --argjson max "$MAXBYTES" \
+  --argjson chunk "$CHUNK_JSON" '
+  { ns: $ns, slot: $slot, url: $url, dryRun: $dry, skipEmbeddings: $noemb }
+  | ( if ($max|tonumber) > 0 then . + { maxFileBytes: ($max|tonumber) } else . end )
+  | ( if ( ($chunk|type)=="object" and ($chunk|length)>0 ) then . + { chunk: $chunk } else . end )
+' \
+| curl -fsS -X POST "$BASE/api/ingest/pdf" \
+  -H "content-type: application/json" -H "x-admin-key: $ADMIN_KEY" \
+  --data-binary @- \
+| jq '{ok, ns, slot, url, dryRun, pages, textChunks, textInserted, textUpdated, unchanged, embedWritten, ms}'
diff --git a/apps/web/scripts/ingest_pdf_file.sh b/apps/web/scripts/ingest_pdf_file.sh
new file mode 100755
index 0000000..6d5a05c
--- /dev/null
+++ b/apps/web/scripts/ingest_pdf_file.sh
@@ -0,0 +1,66 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Использование:
+#   ./scripts/ingest_pdf_file.sh NS SLOT /abs/path/to/file.pdf           # обычная загрузка
+#   ./scripts/ingest_pdf_file.sh NS SLOT /abs/path/to/file.pdf --dry     # dry-run
+#   ./scripts/ingest_pdf_file.sh NS SLOT /abs/path/to/file.pdf --no-emb  # без эмбеддингов
+#   ./scripts/ingest_pdf_file.sh NS SLOT /abs/path/to/file.pdf --chars 1200 --overlap 180
+# Требуется окружение: BASE, ADMIN_KEY (как для остальных скриптов)
+
+: "${BASE:?need BASE like http://localhost:3000}"
+: "${ADMIN_KEY:?need ADMIN_KEY}"
+
+NS="${1:?need ns}"
+SLOT="${2:?need slot}"
+PDF_PATH="${3:?need absolute path to local PDF}"
+shift 3 || true
+
+if [[ ! -f "$PDF_PATH" ]]; then
+  echo "File not found: $PDF_PATH" >&2
+  exit 1
+fi
+
+DRY=false
+NOEMB=false
+MAXBYTES=0
+CHARS=0
+OVERLAP=0
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --dry) DRY=true ;;
+    --no-emb) NOEMB=true ;;
+    --max) shift; MAXBYTES="${1:?need number}" ;;
+    --chars) shift; CHARS="${1:?need number}" ;;
+    --overlap) shift; OVERLAP="${1:?need number}" ;;
+    *) echo "Unknown flag: $1" >&2; exit 2 ;;
+  esac
+  shift || true
+done
+
+make_chunk_json() {
+  local c="$1" o="$2"
+  jq -n --argjson c "${c:-0}" --argjson o "${o:-0}" '
+    (if $c>0 then {chars:$c} else {} end)
+    + (if $o>0 then {overlap:$o} else {} end)
+  '
+}
+
+CHUNK_JSON="$(make_chunk_json "$CHARS" "$OVERLAP")"
+URL="file://$PDF_PATH"
+
+jq -n \
+  --arg ns "$NS" --arg slot "$SLOT" --arg url "$URL" \
+  --argjson dry "$([[ "$DRY" == true ]] && echo true || echo false)" \
+  --argjson noemb "$([[ "$NOEMB" == true ]] && echo true || echo false)" \
+  --argjson max "$MAXBYTES" \
+  --argjson chunk "$CHUNK_JSON" '
+  { ns: $ns, slot: $slot, url: $url, dryRun: $dry, skipEmbeddings: $noemb }
+  | ( if ($max|tonumber) > 0 then . + { maxFileBytes: ($max|tonumber) } else . end )
+  | ( if ( ($chunk|type)=="object" and ($chunk|length)>0 ) then . + { chunk: $chunk } else . end )
+' \
+| curl -fsS -X POST "$BASE/api/ingest/pdf" \
+  -H "content-type: application/json" -H "x-admin-key: $ADMIN_KEY" \
+  --data-binary @- \
+| jq '{ok, ns, slot, url, dryRun, pages, textChunks, textInserted, textUpdated, unchanged, embedWritten, ms}'
diff --git a/apps/web/scripts/ingest_url.sh b/apps/web/scripts/ingest_url.sh
new file mode 100755
index 0000000..996ee10
--- /dev/null
+++ b/apps/web/scripts/ingest_url.sh
@@ -0,0 +1,21 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Использование:
+#   ./scripts/ingest_url.sh rebecca/army/refs staging https://www.rfc-editor.org/rfc/rfc2616
+# Требования: BASE, X_ADMIN_KEY в окружении.
+
+: "${BASE:?need BASE like http://localhost:3000}"
+: "${X_ADMIN_KEY:?need X_ADMIN_KEY from .env.local}"
+
+NS="${1:?need ns}"
+SLOT="${2:?need slot}"
+URL="${3:?need url}"
+
+jq -n --arg ns "$NS" --arg slot "$SLOT" --arg url "$URL" \
+  '{ ns:$ns, slot:$slot, urls:[$url], followRedirects:true }' \
+| curl -sS -X POST "$BASE/api/ingest/url" \
+  -H "content-type: application/json" \
+  -H "x-admin-key: $X_ADMIN_KEY" \
+  --data-binary @-
+echo
diff --git a/apps/web/scripts/retrieve.sh b/apps/web/scripts/retrieve.sh
new file mode 100755
index 0000000..5a6603e
--- /dev/null
+++ b/apps/web/scripts/retrieve.sh
@@ -0,0 +1,98 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Использование:
+#   ./scripts/retrieve.sh "HTTP methods" rebecca/army/refs staging 5 200
+#   ./scripts/retrieve.sh --lite "HTTP" rebecca/army/refs staging 5 100
+#   ./scripts/retrieve.sh --max 300 "HTTP methods" rebecca/army/refs staging 5 200
+#
+# Требуется:
+#   BASE=http://localhost:3000
+
+LITE=false
+MAX_CHARS=0
+
+# Флаги: --lite, --max N
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --lite) LITE=true; shift ;;
+    --max)
+      shift
+      [[ $# -gt 0 ]] || { echo "Expected number after --max" >&2; exit 2; }
+      MAX_CHARS="$1"; shift ;;
+    *) break ;;
+  esac
+done
+
+: "${BASE:?need BASE like http://localhost:3000}"
+
+Q="${1:?need query string}"
+NS="${2:?need ns}"
+SLOT="${3:?need slot}"
+TOP="${4:-5}"
+CAND="${5:-200}"
+
+# Формируем include как JSON-массив через jq (без ручных кавычек)
+if [[ "$LITE" == true ]]; then
+  INCLUDE_JSON='["url","title","score"]'
+else
+  INCLUDE_JSON='["url","title","content","score"]'
+fi
+
+REQ="$(jq -n \
+  --arg q "$Q" \
+  --arg ns "$NS" \
+  --arg slot "$SLOT" \
+  --argjson top "$TOP" \
+  --argjson cand "$CAND" \
+  --argjson include "$INCLUDE_JSON" \
+'{
+  q: $q,
+  ns: $ns,
+  slot: $slot,
+  nsMode: "prefix",
+  topK: $top,
+  candidateK: $cand,
+  minSimilarity: 0,
+  include: $include,
+  debugVersion: true
+}')"
+
+echo -e "\n--- REQUEST ---"
+echo "$REQ" | jq .
+
+# Запрос: тело в файл, статус в переменную
+OUT="$(mktemp)"
+STATUS="$(curl -sS -o "$OUT" -w "%{http_code}" -X POST "$BASE/api/retrieve" \
+  -H "content-type: application/json" \
+  --data-binary @<(echo "$REQ"))"
+
+echo -e "\n--- RESPONSE (HTTP $STATUS) ---"
+cat "$OUT" | head -c 1000;  # показываем начало для наглядности
+echo
+
+if [[ "$STATUS" != "200" ]]; then
+  echo -e "\nRequest failed with HTTP $STATUS. Raw body above."
+  exit 1
+fi
+
+# Проверяем, что это объект с полем items
+if ! jq -e 'type=="object" and has("items")' "$OUT" >/dev/null 2>&1; then
+  echo -e "\nUnexpected response (no .items). Raw body:"
+  cat "$OUT"
+  exit 2
+fi
+
+# Парсинг и опциональная обрезка контента
+if [[ "$LITE" == true ]]; then
+  jq '{top: (.items|length), items: [.items[] | {score, url, title}]}' "$OUT"
+else
+  if [[ "$MAX_CHARS" -gt 0 ]]; then
+    jq --argjson max "$MAX_CHARS" \
+      '{top: (.items|length), items: [.items[] | {score, url, title, content: (.content|tostring|.[0:$max])}]}' "$OUT"
+  else
+    jq '{top: (.items|length), items: [.items[] | {score, url, title}]}' "$OUT"
+  fi
+fi
+
+rm -f "$OUT"
diff --git a/apps/web/scripts/smoke_pdf.sh b/apps/web/scripts/smoke_pdf.sh
new file mode 100755
index 0000000..daf3b97
--- /dev/null
+++ b/apps/web/scripts/smoke_pdf.sh
@@ -0,0 +1,49 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Быстрый self-check для /api/ingest/pdf
+# Требуется: BASE, ADMIN_KEY
+: "${BASE:?need BASE like http://localhost:3000}"
+: "${ADMIN_KEY:?need ADMIN_KEY}"
+
+NS="${1:-rebecca/army/refs}"
+SLOT="${2:-staging}"
+PDF_URL="${3:-https://arxiv.org/pdf/1706.03762.pdf}"
+
+echo "[SMOKE:PDF] 1) Dry-run…"
+jq -n --arg ns "$NS" --arg slot "$SLOT" --arg url "$PDF_URL" \
+'{
+  ns:$ns, slot:$slot, url:$url,
+  dryRun:true,
+  chunk:{chars:1200, overlap:180}
+}' \
+| curl -fsS -X POST "$BASE/api/ingest/pdf" \
+  -H "content-type: application/json" -H "x-admin-key: $ADMIN_KEY" \
+  --data-binary @- \
+| jq '{ok, textChunks, ms}'
+
+echo "[SMOKE:PDF] 2) Real ingest (no embeddings)…"
+jq -n --arg ns "$NS" --arg slot "$SLOT" --arg url "$PDF_URL" \
+'{
+  ns:$ns, slot:$slot, url:$url,
+  skipEmbeddings:true,
+  chunk:{chars:1200, overlap:180}
+}' \
+| curl -fsS -X POST "$BASE/api/ingest/pdf" \
+  -H "content-type: application/json" -H "x-admin-key: $ADMIN_KEY" \
+  --data-binary @- \
+| jq '{ok, textInserted, textUpdated, unchanged, embedWritten, ms}'
+
+echo "[SMOKE:PDF] 3) Backfill embeddings…"
+this_ns="$NS" this_slot="$SLOT"
+apps/web/scripts/embed_backfill.sh "$this_ns" "$this_slot" 100 16 || true
+
+echo "[SMOKE:PDF] 4) NULL embeddings by URL…"
+apps/web/scripts/run_admin_sql.sh "
+SELECT COUNT(*) AS nulls_by_url
+FROM chunks
+WHERE ns='${NS}'
+  AND slot='${SLOT}'
+  AND source_id='${PDF_URL}'
+  AND embedding IS NULL;
+"
diff --git a/apps/web/src/app/api/ingest/github/route.ts b/apps/web/src/app/api/ingest/github/route.ts
index 05a2898..e858cec 100644
--- a/apps/web/src/app/api/ingest/github/route.ts
+++ b/apps/web/src/app/api/ingest/github/route.ts
@@ -1,50 +1,43 @@
 // apps/web/src/app/api/ingest/github/route.ts
 import { NextResponse } from "next/server";
 import { embedMany } from "@/lib/embeddings";
-import { upsertMemoriesBatch } from "@/lib/memories";
 import { chunkText, normalizeChunkOpts } from "@/lib/chunking";
 import { retryFetch } from "@/lib/retryFetch";
+import { assertAdmin } from "@/lib/admin";
+import { upsertChunksWithTargets, type IngestDoc } from "@/lib/ingest_upsert";
 
 export const runtime = "nodejs";
 export const dynamic = "force-dynamic";
 
 type Body = {
   ns: string;
-  slot?: string | null;
+  slot?: "staging" | "prod" | string | null;
   kind?: string | null;
+
   owner: string;
   repo: string;
-  ref?: string | null;          // branch or sha
-  path?: string | null;         // subdir filter (prefix)
-  includeExt?: string[] | null; // e.g. [".md",".mdx",".py",".ipynb",".txt"]
-  excludeExt?: string[] | null;
+  ref?: string | null;          // branch / tag / sha
+  path?: string | null;         // префикс каталога (фильтр)
+  includeExt?: string[] | null; // например [".md",".mdx",".py",".ipynb",".txt"]
+  excludeExt?: string[] | null; // что исключить
 
-  // НОВОЕ: пагинация
-  cursor?: number | null;       // смещение в отсортированном списке файлов (0..)
-  limit?: number | null;        // сколько файлов взять сейчас (дефолт 250)
+  cursor?: number | null;       // смещение
+  limit?: number | null;        // кол-во файлов на страницу (<= 250)
 
-  // НОВОЕ: "сухой прогон" — только посчитать/список, без скачивания/эмбеддингов
-  dryRun?: boolean | null;
+  dryRun?: boolean | null;      // только план без записи
+  skipEmbeddings?: boolean | null; // не считать эмбеддинги (для последующего backfill)
 
-  // стандартные опции чанкинга
   chunk?: { chars?: number; overlap?: number };
 };
 
-function assertAdmin(req: Request) {
-  const need = (process.env.X_ADMIN_KEY || "").trim();
-  if (!need) return;
-  const got = (req.headers.get("x-admin-key") || "").trim();
-  if (need && got !== need) throw new Error("unauthorized");
-}
-
 const GH = "https://api.github.com";
 const UA =
   "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36";
 
-// мягкие лимиты на один ВЫЗОВ (страницу)
-const MAX_LIMIT_FILES     = 250;     // максимум файлов за один вызов (страницу)
-const MAX_FILE_BYTES      = 1_000_000; // пропускаем файлы >1 МБ
-const MAX_TOTAL_CHUNKS    = 3000;    // общий лимит чанков на один вызов
+// мягкие лимиты на вызов
+const MAX_LIMIT_FILES  = 250;
+const MAX_FILE_BYTES   = 1_000_000;
+const MAX_TOTAL_CHUNKS = 3000;
 
 async function gh<T = any>(url: string) {
   const headers: Record<string, string> = {
@@ -56,11 +49,50 @@ async function gh<T = any>(url: string) {
   const res = await retryFetch(url, { headers, redirect: "follow" });
   if (!res.ok) {
     const text = await res.text().catch(() => "");
-    throw new Error(`GitHub ${res.status} ${res.statusText}: ${text.slice(0, 300)}`);
+    throw new Error(`GitHub ${res.status} ${res.statusText}: ${text.slice(0,300)}`);
   }
   return (await res.json()) as T;
 }
 
+async function resolveSha(owner: string, repo: string, ref?: string | null): Promise<{ sha: string; usedRef: string }> {
+  const want = (ref || "").trim();
+
+  // пусто → default_branch
+  if (!want) {
+    const info = await gh<{ default_branch: string }>(`${GH}/repos/${owner}/${repo}`);
+    const def = info.default_branch || "master";
+    const head = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/heads/${def}`);
+    return { sha: head.object.sha, usedRef: def };
+  }
+
+  // heads/<ref>
+  try {
+    const head = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/heads/${want}`);
+    return { sha: head.object.sha, usedRef: want };
+  } catch {}
+
+  // tags/<ref>
+  try {
+    const tag = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/tags/${want}`);
+    return { sha: tag.object.sha, usedRef: want };
+  } catch {}
+
+  // refs/<ref>
+  try {
+    const anyRef = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/${want}`);
+    return { sha: anyRef.object.sha, usedRef: want };
+  } catch {}
+
+  // уже sha?
+  if (/^[0-9a-f]{7,40}$/i.test(want)) return { sha: want, usedRef: want };
+
+  // fallback → default_branch
+  const info = await gh<{ default_branch: string }>(`${GH}/repos/${owner}/${repo}`);
+  const def = info.default_branch || "master";
+  const head = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/heads/${def}`);
+  return { sha: head.object.sha, usedRef: def };
+}
+
 function extOf(name: string) {
   const i = name.lastIndexOf(".");
   return i >= 0 ? name.slice(i).toLowerCase() : "";
@@ -76,10 +108,11 @@ function ipynbToText(nb: any): string {
 }
 
 export async function POST(req: Request) {
-  const started = Date.now();
+  const t0 = Date.now();
   let stage = "init";
   try {
     assertAdmin(req);
+
     const {
       ns,
       slot = "staging",
@@ -90,32 +123,25 @@ export async function POST(req: Request) {
       path = "",
       includeExt,
       excludeExt,
-
       cursor = 0,
       limit = MAX_LIMIT_FILES,
       dryRun = false,
-
+      skipEmbeddings = false,
       chunk,
     } = (await req.json()) as Body;
 
     if (!ns || !owner || !repo) {
-      return NextResponse.json({ ok: false, error: "ns, owner, repo required" }, { status: 400 });
+      return NextResponse.json({ ok: false, stage, error: "ns, owner, repo required" }, { status: 400 });
     }
     const lim = Math.max(1, Math.min(Number(limit) || MAX_LIMIT_FILES, MAX_LIMIT_FILES));
     const cur = Math.max(0, Number(cursor) || 0);
+    const opts = normalizeChunkOpts(chunk);
 
-    // 1) определяем commit SHA
+    // 1) ref → sha
     stage = "ref";
-    let sha = "";
-    try {
-      const head = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/heads/${ref}`);
-      sha = head.object.sha;
-    } catch {
-      const anyRef = await gh<{ object: { sha: string } }>(`${GH}/repos/${owner}/${repo}/git/refs/${ref}`);
-      sha = anyRef.object.sha;
-    }
+    const { sha, usedRef } = await resolveSha(owner, repo, ref);
 
-    // 2) получаем дерево файлов, фильтруем и сортируем
+    // 2) дерево + фильтры
     stage = "tree";
     const tree = await gh<{ tree: { path: string; type: string; sha: string }[] }>(
       `${GH}/repos/${owner}/${repo}/git/trees/${sha}?recursive=1`
@@ -125,7 +151,6 @@ export async function POST(req: Request) {
       const e = extOf(name);
       if (includeExt && includeExt.length && !includeExt.includes(e)) return false;
       if (excludeExt && excludeExt.includes(e)) return false;
-      // базовый отсев бинарников/медиа
       if ([".png",".jpg",".jpeg",".gif",".webp",".svg",".pdf",".zip",".tar",".gz",".7z",".mp4",".mp3"].includes(e)) return false;
       return true;
     };
@@ -141,50 +166,51 @@ export async function POST(req: Request) {
     const nextCursor = cur + pageFiles.length < totalFiles ? cur + pageFiles.length : null;
 
     if (dryRun) {
-      // Ничего не скачиваем/не пишем — только план
       return NextResponse.json({
         ok: true,
-        ns, slot, owner, repo, ref,
+        ns, slot, owner, repo, ref: usedRef,
         totalFiles,
         windowStart: cur,
         windowEnd: cur + pageFiles.length - 1,
         pageFiles: pageFiles.length,
         nextCursor,
-        ms: Date.now() - started,
-        preview: pageFiles.slice(0, 10), // маленький список для наглядности
+        ms: Date.now() - t0,
+        preview: pageFiles.slice(0, 10),
       });
     }
 
     if (!pageFiles.length) {
       return NextResponse.json({
         ok: true,
-        ns, slot, owner, repo, ref,
+        ns, slot, owner, repo, ref: usedRef,
         totalFiles,
         windowStart: cur,
         windowEnd: cur - 1,
         pageFiles: 0,
-        chunks: 0,
-        written: 0,
+        textChunks: 0,
+        textInserted: 0,
+        textUpdated: 0,
+        unchanged: 0,
+        embedWritten: 0,
         nextCursor,
-        ms: Date.now() - started,
+        ms: Date.now() - t0,
       });
     }
 
-    // 3) скачиваем контент выбранных файлов и чанк-ним (с ограничениями)
+    // 3) fetch + chunk (с ограничениями)
     stage = "fetch+chunk";
-    const chunksAll: string[] = [];
-    const metas: any[] = [];
-    const opts = normalizeChunkOpts(chunk);
+    const docs: IngestDoc[] = [];
+    let totalChunks = 0;
 
     for (const p of pageFiles) {
-      // метаданные и размер
+      // метаданные (размер)
       const meta = await gh<{ size?: number; path: string }>(
-        `${GH}/repos/${owner}/${repo}/contents/${encodeURIComponent(p)}?ref=${ref}`
+        `${GH}/repos/${owner}/${repo}/contents/${encodeURIComponent(p)}?ref=${usedRef}`
       );
       if ((meta as any)?.size && (meta as any).size > MAX_FILE_BYTES) continue;
 
       const raw = await gh<{ content: string; encoding: string; path: string; size?: number }>(
-        `${GH}/repos/${owner}/${repo}/contents/${encodeURIComponent(p)}?ref=${ref}`
+        `${GH}/repos/${owner}/${repo}/contents/${encodeURIComponent(p)}?ref=${usedRef}`
       );
 
       let text = "";
@@ -200,61 +226,106 @@ export async function POST(req: Request) {
       if (!text) continue;
 
       const parts = chunkText(text, opts);
-      for (const part of parts) {
-        chunksAll.push(part);
-        metas.push({
+      if (!parts.length) continue;
+
+      const sourceUrl = `https://github.com/${owner}/${repo}/blob/${usedRef}/${raw.path}`;
+      const sourceId  = `github:${owner}/${repo}@${usedRef}:${raw.path}`;
+
+      const doc: IngestDoc = {
+        ns,
+        slot,
+        source_id: sourceId,
+        url: sourceUrl,
+        title: null,
+        published_at: null,
+        source_type: "github",
+        kind: kind || "github",
+        doc_metadata: {
           source_type: "github",
-          owner, repo, ref, path: raw.path,
+          owner, repo, ref: usedRef, path: raw.path,
           chunk: opts,
-          chunk_chars: part.length,
-        });
-        if (chunksAll.length >= MAX_TOTAL_CHUNKS) break;
-      }
-      if (chunksAll.length >= MAX_TOTAL_CHUNKS) break;
+          chunk_total: parts.length,
+        },
+        chunks: parts.map((content, i) => ({
+          content,
+          chunk_no: i,
+          metadata: {
+            source_type: "github",
+            owner, repo, ref: usedRef, path: raw.path,
+            chunk: opts,
+            chunk_chars: content.length,
+          },
+        })),
+      };
+
+      docs.push(doc);
+      totalChunks += parts.length;
+      if (totalChunks >= MAX_TOTAL_CHUNKS) break;
     }
 
-    if (!chunksAll.length) {
+    if (!docs.length) {
       return NextResponse.json({
         ok: true,
-        ns, slot, owner, repo, ref,
+        ns, slot, owner, repo, ref: usedRef,
         totalFiles,
         windowStart: cur,
         windowEnd: cur + pageFiles.length - 1,
         pageFiles: pageFiles.length,
-        chunks: 0,
-        written: 0,
+        textChunks: 0,
+        textInserted: 0,
+        textUpdated: 0,
+        unchanged: 0,
+        embedWritten: 0,
         nextCursor,
-        ms: Date.now() - started,
+        ms: Date.now() - t0,
       });
     }
 
-    // 4) эмбеддинги и запись
-    stage = "embed";
-    const vectors = await embedMany(chunksAll);
-
-    stage = "db";
-    const records = chunksAll.map((content, i) => ({
-      kind: kind || "github",
-      ns, slot,
-      content,
-      embedding: vectors[i],
-      metadata: metas[i],
-    }));
-
-    // Важно: у тебя upsertMemoriesBatch возвращает number (сколько записей сделано)
-    const written: number = await upsertMemoriesBatch(records);
+    // 4) upsert чанков с таргетами
+    stage = "db-upsert";
+    const { inserted, updated, targets, unchanged } = await upsertChunksWithTargets(docs);
+
+    // 5) эмбеддинги только по target’ам (если не запретили)
+    let embedWritten = 0;
+    if (!skipEmbeddings && targets.length) {
+      stage = "embed";
+      const vectors = await embedMany(targets.map(t => t.content));
+      stage = "db-embed";
+      // батчево: один UPDATE per id
+      // (простая петля; можно сделать bulk через UNNEST, но для простоты так)
+      const { pool } = await import("@/lib/pg");
+      const client = await pool.connect();
+      try {
+        await client.query("BEGIN");
+        for (let i = 0; i < targets.length; i++) {
+          const id = targets[i].id;
+          const v  = vectors[i];
+          await client.query(`UPDATE chunks SET embedding = $1, updated_at = NOW() WHERE id = $2`, [v, id]);
+          embedWritten += 1;
+        }
+        await client.query("COMMIT");
+      } catch (e) {
+        await client.query("ROLLBACK");
+        throw e;
+      } finally {
+        client.release();
+      }
+    }
 
     return NextResponse.json({
       ok: true,
-      ns, slot, owner, repo, ref,
+      ns, slot, owner, repo, ref: usedRef,
       totalFiles,
       windowStart: cur,
       windowEnd: cur + pageFiles.length - 1,
       pageFiles: pageFiles.length,
-      chunks: chunksAll.length,
-      written,
+      textChunks: totalChunks,
+      textInserted: inserted,
+      textUpdated: updated,
+      unchanged,
+      embedWritten,
       nextCursor,
-      ms: Date.now() - started,
+      ms: Date.now() - t0,
     });
   } catch (e: any) {
     return NextResponse.json({ ok: false, stage, error: e?.message || String(e) }, { status: 500 });
diff --git a/apps/web/src/app/api/ingest/pdf/route.ts b/apps/web/src/app/api/ingest/pdf/route.ts
index 905d9da..7d46a19 100644
--- a/apps/web/src/app/api/ingest/pdf/route.ts
+++ b/apps/web/src/app/api/ingest/pdf/route.ts
@@ -1,7 +1,10 @@
+// apps/web/src/app/api/ingest/pdf/route.ts
 import { NextResponse } from "next/server";
-import { embedMany } from "@/lib/embeddings";
-import { upsertChunks, type IngestDoc } from "@/lib/ingest_upsert";
+import { assertAdmin } from "@/lib/admin";
 import { chunkText, normalizeChunkOpts } from "@/lib/chunking";
+import { upsertChunksWithTargets, type IngestDoc } from "@/lib/ingest_upsert";
+import { embedMany } from "@/lib/embeddings";
+import { pool } from "@/lib/pg";
 import { retryFetch } from "@/lib/retryFetch";
 
 export const runtime = "nodejs";
@@ -11,17 +14,16 @@ type Body = {
   ns: string;
   url: string;
   slot?: "staging" | "prod" | string | null;
-  kind?: string | null;
+  kind?: string | null;                 // "pdf" по умолчанию
   chunk?: { chars?: number; overlap?: number };
-  maxFileBytes?: number | null;
+  maxFileBytes?: number | null;         // лимит на размер PDF
+  dryRun?: boolean;                     // только посчитать чанки, без записи в БД
+  skipEmbeddings?: boolean;             // пропустить расчёт эмбеддингов
 };
 
-function assertAdmin(req: Request) {
-  const need = (process.env.X_ADMIN_KEY || "").trim();
-  if (!need) return;
-  const got = (req.headers.get("x-admin-key") || "").trim();
-  if (need && got !== need) throw new Error("unauthorized");
-}
+// user-agent пригодится для некоторых источников
+const UA =
+  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36";
 
 async function importPdfParse(): Promise<(buf: Buffer) => Promise<any>> {
   const mod: any = await import("pdf-parse");
@@ -34,8 +36,9 @@ async function fetchAsBuffer(url: string, maxBytes?: number): Promise<Buffer> {
     const { readFile } = await import("node:fs/promises");
     return readFile(url.slice("file://".length));
   }
-  const r = await retryFetch(url, { redirect: "follow" });
+  const r = await retryFetch(url, { redirect: "follow", headers: { "User-Agent": UA, Accept: "application/pdf,*/*" } });
   if (!r.ok) throw new Error(`fetch failed: ${r.status} ${r.statusText}`);
+  // если тело доступно стримом — читаем с лимитом
   const reader = r.body?.getReader();
   if (!reader) return Buffer.from(await r.arrayBuffer());
   const parts: Uint8Array[] = [];
@@ -52,14 +55,19 @@ async function fetchAsBuffer(url: string, maxBytes?: number): Promise<Buffer> {
   return Buffer.concat(parts);
 }
 
+// На случай, если pdf-parse не справился или источник капризный — Jina рендер
 async function fetchPdfViaJina(url: string): Promise<string> {
   const enc = encodeURI(url).replace(/^https?:\/\//, "");
   const jina = `https://r.jina.ai/https://${enc}`;
-  const r = await retryFetch(jina, { redirect: "follow" });
+  const r = await retryFetch(jina, { redirect: "follow", headers: { "User-Agent": UA } });
   if (!r.ok) throw new Error(`Jina ${r.status} ${r.statusText}`);
   return await r.text();
 }
 
+function toVectorLiteral(vec: number[]): string {
+  return `[${vec.join(",")}]`;
+}
+
 export async function POST(req: Request) {
   const t0 = Date.now();
   let stage: string = "init";
@@ -73,6 +81,8 @@ export async function POST(req: Request) {
     const kind = (body.kind || "pdf");
     const opts = normalizeChunkOpts(body.chunk);
     const MAXB = Number.isFinite(Number(body.maxFileBytes)) ? Math.max(50_000, Number(body.maxFileBytes)) : undefined;
+    const dryRun = !!body.dryRun;
+    const skipEmb = !!body.skipEmbeddings;
 
     if (!ns || !url) {
       return NextResponse.json({ ok: false, stage, error: "ns and url are required" }, { status: 400 });
@@ -90,25 +100,40 @@ export async function POST(req: Request) {
       const pdfParse = await importPdfParse();
       stage = "pdf-parse";
       const out = await pdfParse(buf);
-      text  = (out?.text || "").trim();
+      text  = (out?.text || "").replace(/\s+/g, " ").trim();
       pages = out?.numpages;
     } catch {
-      // сетевой fallback (alphaxiv и пр.)
       stage = "fallback-jina";
-      text = (await fetchPdfViaJina(url)).trim();
+      text = (await fetchPdfViaJina(url)).replace(/\s+/g, " ").trim();
     }
 
-    if (!text) throw new Error("empty text extracted from PDF");
+    if (!text) {
+      return NextResponse.json({
+        ok: true, ns, slot, url, dryRun,
+        pages: pages ?? null,
+        textChunks: 0, textInserted: 0, textUpdated: 0, unchanged: 0, embedWritten: 0,
+        ms: Date.now() - t0,
+      });
+    }
 
     stage = "chunk";
     const parts = chunkText(text, opts);
-    if (!parts.length) throw new Error("no chunks produced");
+    const textChunks = parts.length;
+
+    if (dryRun) {
+      return NextResponse.json({
+        ok: true, ns, slot, url, dryRun: true,
+        pages: pages ?? null,
+        textChunks, textInserted: 0, textUpdated: 0, unchanged: 0, embedWritten: 0,
+        ms: Date.now() - t0,
+      });
+    }
 
-    stage = "db";
+    stage = "db-upsert";
     const doc: IngestDoc = {
       ns,
       slot,
-      source_id: url,
+      source_id: url,              // единая политика: source_id = URL
       url,
       title: null,
       published_at: null,
@@ -134,19 +159,35 @@ export async function POST(req: Request) {
       })),
     };
 
-    // upsert
-    await upsertChunks([doc]);
+    const { inserted, updated, unchanged, targets } = await upsertChunksWithTargets([doc]);
+
+    stage = "embeddings";
+    let embedWritten = 0;
+    if (!skipEmb && targets.length) {
+      const contents = targets.map(t => t.content);
+      const vectors  = await embedMany(contents); // проверяет EMBED_DIMS=1536
+      for (let i = 0; i < targets.length; i++) {
+        const id  = targets[i].id;
+        const lit = toVectorLiteral(vectors[i]);
+        await pool.query(`UPDATE chunks SET embedding = $1::vector, updated_at = NOW() WHERE id = $2`, [lit, id]);
+        embedWritten += 1;
+      }
+    }
 
     return NextResponse.json({
-      ok: true, ns, slot, url,
+      ok: true, ns, slot, url, dryRun: false,
       pages: pages ?? null,
-      chunks: parts.length,
+      textChunks, textInserted: inserted, textUpdated: updated, unchanged, embedWritten,
       ms: Date.now() - t0,
     });
   } catch (e: any) {
     return NextResponse.json(
       { ok: false, stage, error: e?.message || String(e) },
-      { status: 500 }
+      { status: e?.message === "unauthorized" ? 401 : 500 }
     );
   }
 }
+
+export function GET() {
+  return NextResponse.json({ error: "Method Not Allowed" }, { status: 405 });
+}
diff --git a/apps/web/src/lib/admin.ts b/apps/web/src/lib/admin.ts
new file mode 100644
index 0000000..2529735
--- /dev/null
+++ b/apps/web/src/lib/admin.ts
@@ -0,0 +1,21 @@
+// apps/web/src/lib/admin.ts
+import { NextRequest } from "next/server";
+
+/**
+ * Проверяем админ-доступ по заголовку x-admin-key.
+ * Допускаем ИЛИ ADMIN_KEY, ИЛИ X_ADMIN_KEY из окружения (если заданы оба — любой из них).
+ */
+export function assertAdmin(req: NextRequest | Request): void {
+  const wantA = (process.env.ADMIN_KEY || "").trim();
+  const wantB = (process.env.X_ADMIN_KEY || "").trim(); // на проектах уже используется для ingest
+  const got = (req.headers.get("x-admin-key") || "").trim();
+
+  // если оба пустые — считаем, что админ-защита не настроена (не безопасно, но не блокируем локальную разработку)
+  if (!wantA && !wantB) return;
+
+  const ok =
+    (!!wantA && got === wantA) ||
+    (!!wantB && got === wantB);
+
+  if (!ok) throw new Error("unauthorized");
+}
diff --git a/apps/web/src/lib/ingest_upsert.ts b/apps/web/src/lib/ingest_upsert.ts
index dd56aa6..81fc902 100644
--- a/apps/web/src/lib/ingest_upsert.ts
+++ b/apps/web/src/lib/ingest_upsert.ts
@@ -1,207 +1,179 @@
 // apps/web/src/lib/ingest_upsert.ts
 import { pool } from "@/lib/pg";
-import { createHash } from "crypto";
-import { embedQuery } from "@/lib/embeddings";
+import crypto from "crypto";
 
 export type IngestChunk = {
   content: string;
   chunk_no: number;
-  metadata?: Record<string, any>;
+  metadata: Record<string, any>;
 };
 
 export type IngestDoc = {
   ns: string;
-  slot: "staging" | "prod";
-  source_id: string;
-  url?: string | null;
-  title?: string | null;
-  published_at?: string | null;
-  source_type?: string | null;
-  kind?: string | null;
+  slot: "staging" | "prod" | (string & {});
+  source_id: string | null;
+  url: string | null;
+  title: string | null;
+  published_at: string | null;
+  source_type: string | null;
+  kind: string | null;
+  doc_metadata: Record<string, any>;
   chunks: IngestChunk[];
-  doc_metadata?: Record<string, any>;
 };
 
-const hashText = (t: string) => createHash("sha256").update(t).digest("hex");
+export type UpsertResult = { inserted: number; updated: number };
+export type UpsertTargetsResult = UpsertResult & {
+  targets: Array<{ id: string; content: string }>;
+  unchanged: number; // НОВОЕ: сколько столкнулись, но контент не изменился
+};
 
-async function embedMany(texts: string[], concurrency = 8): Promise<number[][]> {
-  const out: number[][] = new Array(texts.length);
-  let i = 0;
-  const worker = async () => {
-    while (true) {
-      const idx = i++;
-      if (idx >= texts.length) break;
-      out[idx] = await embedQuery(texts[idx]);
-    }
-  };
-  await Promise.all(Array.from({ length: Math.min(concurrency, texts.length) }, worker));
-  return out;
+function sha1(text: string): string {
+  return crypto.createHash("sha1").update(text).digest("hex");
 }
 
-const sanitize = (s: unknown, max = 10_000): string | null => {
-  if (typeof s !== "string") return null;
-  const t = s.replace(/\u0000/g, "").trim();
-  if (!t) return null;
-  return t.length > max ? t.slice(0, max) : t;
-};
+function makeSnippet(text: string, max = 480): string {
+  const s = text.replace(/\s+/g, " ").trim();
+  return s.length <= max ? s : s.slice(0, max);
+}
 
-const EXPECTED_DIM = 1536;
+/** Прежний upsert без таргетов — оставляем для совместимости */
+export async function upsertChunks(docs: IngestDoc[]): Promise<UpsertResult> {
+  if (!Array.isArray(docs) || docs.length === 0) return { inserted: 0, updated: 0 };
 
-export async function upsertChunks(docs: IngestDoc[]) {
-  if (!docs.length) return { inserted: 0, updated: 0, total: 0 };
+  const client = await pool.connect();
+  try {
+    await client.query("BEGIN");
+    let inserted = 0, updated = 0;
 
-  for (const d of docs) {
-    if (!d.ns?.trim()) throw new Error("upsertChunks: ns is required");
-    if (d.slot !== "staging" && d.slot !== "prod") {
-      throw new Error(`upsertChunks: slot must be 'staging'|'prod', got '${d.slot}'`);
-    }
-    if (!d.source_id?.trim()) throw new Error("upsertChunks: source_id is required");
-    if (!Array.isArray(d.chunks) || d.chunks.length === 0) {
-      throw new Error(`upsertChunks: empty chunks for source_id=${d.source_id}`);
-    }
-  }
-
-  const flat: Array<{
-    ns: string;
-    slot: "staging" | "prod";
-    content: string;
-    url: string | null;
-    title: string | null;
-    snippet: string | null;
-    published_at: string | null;
-    source_type: string | null;
-    kind: string | null;
-    metadata: Record<string, any>;
-    content_hash: string;
-    source_id: string;
-    chunk_no: number;
-  }> = [];
-
-  for (const d of docs) {
-    const commonMeta = d.doc_metadata || {};
-    for (const ch of d.chunks) {
-      const content = sanitize(ch.content, 50_000);
-      if (!content || content.length < 5) continue;
-
-      const snippet = content.slice(0, 300);
-      flat.push({
-        ns: d.ns.trim(),
-        slot: d.slot,
-        content,
-        url: sanitize(d.url || null, 2048),
-        title: sanitize(d.title || null, 512),
-        snippet,
-        published_at: d.published_at || null,
-        source_type: d.source_type || null,
-        kind: d.kind || null,
-        metadata: { ...commonMeta, ...(ch.metadata || {}) },
-        content_hash: hashText(content),
-        source_id: d.source_id,
-        chunk_no: ch.chunk_no,
-      });
+    const textInsert = `
+      INSERT INTO chunks (
+        ns, slot, content, url, title, snippet, published_at,
+        source_type, kind, metadata, content_hash, source_id, chunk_no, created_at, updated_at
+      ) VALUES (
+        $1, $2, $3, $4, $5, $6, $7,
+        $8, $9, $10, $11, $12, $13, NOW(), NOW()
+      )
+      ON CONFLICT (ns, slot, source_id, chunk_no) DO UPDATE SET
+        content      = EXCLUDED.content,
+        url          = EXCLUDED.url,
+        title        = EXCLUDED.title,
+        snippet      = EXCLUDED.snippet,
+        published_at = EXCLUDED.published_at,
+        source_type  = EXCLUDED.source_type,
+        kind         = EXCLUDED.kind,
+        metadata     = EXCLUDED.metadata,
+        content_hash = EXCLUDED.content_hash,
+        updated_at   = NOW()
+      RETURNING xmax = 0 AS inserted
+    `;
+
+    for (const d of docs) {
+      const ns = d.ns, slot = d.slot, docMeta = d.doc_metadata ?? {};
+      for (const ch of d.chunks) {
+        const content = String(ch.content ?? ""); if (!content) continue;
+        const rowMeta = { ...(ch.metadata ?? {}), doc: docMeta };
+        const snippet = makeSnippet(content);
+        const hash = sha1(content);
+
+        const params = [ns, slot, content, d.url ?? null, d.title ?? null, snippet,
+                        d.published_at ?? null, d.source_type ?? null, d.kind ?? null,
+                        rowMeta, hash, d.source_id ?? null, ch.chunk_no];
+
+        const res = await client.query<{ inserted: boolean }>(textInsert, params);
+        if (res.rows[0]?.inserted) inserted += 1; else updated += 1;
+      }
     }
-  }
-
-  if (!flat.length) return { inserted: 0, updated: 0, total: 0 };
-
-  const embeddings = await embedMany(flat.map(f => f.content));
+    await client.query("COMMIT");
+    return { inserted, updated };
+  } catch (e) {
+    await client.query("ROLLBACK"); throw e;
+  } finally { client.release(); }
+}
 
-  if (embeddings.length !== flat.length) {
-    throw new Error(`upsertChunks: embeddings count mismatch: got ${embeddings.length}, expected ${flat.length}`);
-  }
-  const firstBad = embeddings.findIndex(v => !Array.isArray(v) || v.length !== EXPECTED_DIM);
-  if (firstBad >= 0) {
-    throw new Error(`upsertChunks: embedding dim != ${EXPECTED_DIM} at index ${firstBad} (len=${embeddings[firstBad]?.length})`);
-  }
+/**
+ * Новый upsert с таргетами и подсчётом unchanged.
+ * Логика:
+ *  • Если запись с ключом (ns,slot,source_id,chunk_no) есть и content_hash совпадает → считаем как unchanged и пропускаем.
+ *  • Иначе INSERT ... ON CONFLICT ... DO UPDATE (только при смене hash) с RETURNING — для целей эмбеддинга.
+ */
+export async function upsertChunksWithTargets(docs: IngestDoc[]): Promise<UpsertTargetsResult> {
+  if (!Array.isArray(docs) || docs.length === 0) return { inserted: 0, updated: 0, targets: [], unchanged: 0 };
 
   const client = await pool.connect();
   try {
     await client.query("BEGIN");
-    await client.query("SET LOCAL statement_timeout = 0");
-    await client.query("SET LOCAL application_name = 'ingest_upsert'");
-
-    let inserted = 0, updated = 0;
-
-    const BATCH = 500;
-    for (let off = 0; off < flat.length; off += BATCH) {
-      const slice = flat.slice(off, off + BATCH);
-      const vecs  = embeddings.slice(off, off + BATCH);
-
-      const rowsSql: string[] = [];
-      const params: any[] = [];
-      slice.forEach((r, i) => {
-        const base = i * 14;
-        rowsSql.push(`(
-          $${base + 1}::text,
-          $${base + 2}::text,
-          $${base + 3}::text,
-          $${base + 4}::vector,
-          $${base + 5}::text,
-          $${base + 6}::text,
-          $${base + 7}::text,
-          $${base + 8}::timestamptz,
-          $${base + 9}::text,
-          $${base +10}::text,
-          $${base +11}::jsonb,
-          $${base +12}::text,
-          $${base +13}::text,
-          $${base +14}::int
-        )`);
-        params.push(
-          r.ns,
-          r.slot,
-          r.content,
-          `[${vecs[i].join(",")}]`, // Pass embedding as a string representation of a vector
-          r.url,
-          r.title,
-          r.snippet,
-          r.published_at,
-          r.source_type,
-          r.kind,
-          JSON.stringify(r.metadata ?? {}),
-          r.content_hash,
-          r.source_id,
-          r.chunk_no,
-        );
-      });
-
-      const sql = `
-        INSERT INTO chunks
-          (ns, slot, content, embedding, url, title, snippet, published_at,
-           source_type, kind, metadata, content_hash, source_id, chunk_no)
-        VALUES ${rowsSql.join(",\n")}
-        ON CONFLICT (ns, slot, COALESCE(source_id,''), chunk_no)
-        DO UPDATE SET
-          content       = EXCLUDED.content,
-          embedding     = EXCLUDED.embedding,
-          url           = EXCLUDED.url,
-          title         = EXCLUDED.title,
-          snippet       = EXCLUDED.snippet,
-          published_at  = EXCLUDED.published_at,
-          source_type   = EXCLUDED.source_type,
-          kind          = EXCLUDED.kind,
-          metadata      = EXCLUDED.metadata,
-          content_hash  = EXCLUDED.content_hash,
-          updated_at    = now()
+    let inserted = 0, updated = 0, unchanged = 0;
+    const targets: Array<{ id: string; content: string }> = [];
+
+    const textSelectExisting = `
+      SELECT id, content_hash
+      FROM chunks
+      WHERE ns = $1 AND slot = $2 AND source_id IS NOT DISTINCT FROM $3 AND chunk_no = $4
+      LIMIT 1
+    `;
+
+    const textInsertReturn = `
+      WITH up AS (
+        INSERT INTO chunks (
+          ns, slot, content, url, title, snippet, published_at,
+          source_type, kind, metadata, content_hash, source_id, chunk_no, created_at, updated_at
+        ) VALUES (
+          $1, $2, $3, $4, $5, $6, $7,
+          $8, $9, $10, $11, $12, $13, NOW(), NOW()
+        )
+        ON CONFLICT (ns, slot, source_id, chunk_no) DO UPDATE SET
+          content      = EXCLUDED.content,
+          url          = EXCLUDED.url,
+          title        = EXCLUDED.title,
+          snippet      = EXCLUDED.snippet,
+          published_at = EXCLUDED.published_at,
+          source_type  = EXCLUDED.source_type,
+          kind         = EXCLUDED.kind,
+          metadata     = EXCLUDED.metadata,
+          content_hash = EXCLUDED.content_hash,
+          updated_at   = NOW()
         WHERE chunks.content_hash IS DISTINCT FROM EXCLUDED.content_hash
-        RETURNING (xmax = 0) AS inserted_flag;
-      `;
-
-      const res = await client.query<{ inserted_flag: boolean }>(sql, params);
-      for (const row of res.rows) {
-        if (row.inserted_flag) inserted++; else updated++;
+        RETURNING id, xmax = 0 AS inserted, content AS new_content
+      )
+      SELECT id, inserted, new_content FROM up
+    `;
+
+    for (const d of docs) {
+      const ns = d.ns, slot = d.slot, docMeta = d.doc_metadata ?? {};
+
+      for (const ch of d.chunks) {
+        const content = String(ch.content ?? ""); if (!content) continue;
+        const rowMeta = { ...(ch.metadata ?? {}), doc: docMeta };
+        const snippet = makeSnippet(content);
+        const hash = sha1(content);
+
+        // 1) Быстрый чек: есть ли запись и совпадает ли hash → unchanged
+        const existing = await client.query<{ id: string; content_hash: string }>(textSelectExisting, [
+          ns, slot, d.source_id ?? null, ch.chunk_no
+        ]);
+        if (existing.rows.length && existing.rows[0].content_hash === hash) {
+          unchanged += 1;
+          continue; // ничего не делаем
+        }
+
+        // 2) Иначе — вставка/апдейт с возвратом целей
+        const params = [ns, slot, content, d.url ?? null, d.title ?? null, snippet,
+                        d.published_at ?? null, d.source_type ?? null, d.kind ?? null,
+                        rowMeta, hash, d.source_id ?? null, ch.chunk_no];
+
+        const res = await client.query<{ id: string; inserted: boolean; new_content: string }>(textInsertReturn, params);
+
+        // в rows — только новые либо реально обновлённые
+        for (const row of res.rows) {
+          if (row.inserted) inserted += 1; else updated += 1;
+          targets.push({ id: row.id, content: row.new_content });
+        }
       }
     }
 
     await client.query("COMMIT");
-    return { inserted, updated, total: flat.length };
-  } catch (e: any) {
-    await client.query("ROLLBACK");
-    const msg = e?.message || String(e);
-    throw new Error(`upsertChunks failed: ${msg}`);
-  } finally {
-    client.release();
-  }
+    return { inserted, updated, targets, unchanged };
+  } catch (e) {
+    await client.query("ROLLBACK"); throw e;
+  } finally { client.release(); }
 }
-
-
diff --git a/scripts/ingest_github_paged.sh b/scripts/ingest_github_paged.sh
old mode 100644
new mode 100755
index bc860b4..02a2f85
--- a/scripts/ingest_github_paged.sh
+++ b/scripts/ingest_github_paged.sh
@@ -1,3 +1,4 @@
+# scripts/ingest_github_paged.sh
 #!/usr/bin/env bash
 set -euo pipefail
 
@@ -12,19 +13,32 @@ NS="${3:-rebecca/army/refs}"
 SLOT="${4:-staging}"
 REF="${5:-main}"
 LIMIT="${6:-250}"
-INCLUDE_EXT_JSON="${7:-[".md",".mdx",".py",".ipynb",".txt"]}"
+
+# Безопасный дефолт JSON-массива расширений (должен быть валидным JSON!)
+DEFAULT_INCLUDE='[".md",".mdx",".py",".ipynb",".txt"]'
+INCLUDE_EXT_JSON="${7:-$DEFAULT_INCLUDE}"
 
 APP=~/projects/freya-rebecca/apps/web
-BASE=http://localhost:3000
-ADM="x-admin-key: $(grep -E '^X_ADMIN_KEY=' "$APP/.env.local" | cut -d= -f2-)"
+BASE="${BASE:-http://localhost:3000}"
 
-# checkpoint для продолжения с места остановки
+# Заголовок авторизации: берём X_ADMIN_KEY из .env.local, если ADMIN_KEY не задан снаружи
+if [[ -z "${ADMIN_KEY:-}" ]]; then
+  if [[ -f "$APP/.env.local" ]]; then
+    XAK="$(grep -E '^X_ADMIN_KEY=' "$APP/.env.local" | cut -d= -f2- || true)"
+    ADMIN_KEY="${XAK:-}"
+  else
+    ADMIN_KEY=""
+  fi
+fi
+ADM="x-admin-key: ${ADMIN_KEY}"
+
+# checkpoint для возобновления
 CKPT_DIR="$APP/.ingest_checkpoints"
 mkdir -p "$CKPT_DIR"
 CKPT="$CKPT_DIR/${OWNER}_${REPO}_${REF}.cursor"
 
 if [[ -f "$CKPT" ]]; then
-  CURSOR=$(cat "$CKPT")
+  CURSOR="$(cat "$CKPT")"
 else
   CURSOR=0
 fi
@@ -34,10 +48,11 @@ TOTAL=""
 PAGE=0
 
 while true; do
+  # Формируем корректное JSON-тело. includeExt должен быть JSON-массивом!
   REQ=$(jq -n \
     --arg ns "$NS" --arg slot "$SLOT" \
     --arg owner "$OWNER" --arg repo "$REPO" --arg ref "$REF" \
-    --argjson cursor $CURSOR --argjson limit $LIMIT \
+    --argjson cursor "$CURSOR" --argjson limit "$LIMIT" \
     --argjson includeExt "$INCLUDE_EXT_JSON" \
     '{ns:$ns,slot:$slot,owner:$owner,repo:$repo,ref:$ref,includeExt:$includeExt,cursor:$cursor,limit:$limit}')
 
@@ -56,13 +71,18 @@ while true; do
   WINDOW_START=$(echo "$RESP" | jq -r '.windowStart')
   WINDOW_END=$(echo "$RESP" | jq -r '.windowEnd')
   PAGE_FILES=$(echo "$RESP" | jq -r '.pageFiles')
-  CHUNKS=$(echo "$RESP" | jq -r '.chunks')
-  WRITTEN=$(echo "$RESP" | jq -r '.written')
+  CHUNKS=$(echo "$RESP" | jq -r '.chunks // 0')
+
+  # НОВОЕ: берём inserted/updated/unchanged вместо прежнего written
+  INSERTED=$(echo "$RESP" | jq -r '.inserted // 0')
+  UPDATED=$(echo "$RESP" | jq -r '.updated // 0')
+  UNCHANGED=$(echo "$RESP" | jq -r '.unchanged // 0')
+
   MS=$(echo "$RESP" | jq -r '.ms')
   NEXT=$(echo "$RESP" | jq -r '.nextCursor')
 
   PAGE=$((PAGE+1))
-  echo "page #$PAGE files [$WINDOW_START..$WINDOW_END] pageFiles=$PAGE_FILES chunks=$CHUNKS written=$WRITTEN time=${MS}ms"
+  echo "page #$PAGE files [$WINDOW_START..$WINDOW_END] pageFiles=$PAGE_FILES chunks=$CHUNKS ins=$INSERTED upd=$UPDATED same=$UNCHANGED time=${MS}ms"
 
   if [[ "$NEXT" != "null" && -n "$NEXT" ]]; then
     echo -n "$NEXT" > "$CKPT"
-- 
2.43.0

